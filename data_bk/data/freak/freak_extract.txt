The culprit was crime.
It had been rising relentlessly—a graph plotting the crime rate in any American city over recent decades looked like a ski slope in profile—and it seemed now to herald the end of the world as we knew it.
So too had carjacking and crack dealing, robbery and rape.
Violent crime was a gruesome, constant companion.
All the experts were saying so.
He was a scrawny, big-city teenager with a cheap gun in his hand and nothing in his heart but ruthlessness.
In 1995 the criminologist James Alan Fox wrote a report for the U.S. attorney general that grimly detailed the coming spike in murders by teenagers.
In the optimistic scenario, he believed, the rate of teen homicides would rise another 15 percent over the next decade; in the pessimistic scenario, it would more than double.
“The next crime wave will get so bad,” he said, “that it will make 1995 look like the good old days.” Other criminologists, political scientists, and similarly learned forecasters laid out the same horrible future, as did President Clinton.
“We know we’ve got about six years to turn this juvenile crime thing around,” Clinton said, “or our country is going to be living with chaos.
And my successors will not be giving speeches about the wonderful opportunities of the global economy; they’ll be trying to keep body and soul together for people on the streets of these cities.” The smart money was plainly on the criminals.
And then, instead of going up and up and up, crime began to fall.
The crime drop was startling in several respects.
It was ubiquitous, with every category of crime falling in every part of the country.
And it was entirely unanticipated—especially by the very experts who had been predicting the opposite.
The teenage murder rate, instead of rising 100 percent or even 15 percent as James Alan Fox had warned, fell more than 50 percent within five years.
So had the rate of just about every other sort of crime, from assault to car theft.
Even though the experts had failed to anticipate the crime drop—which was in fact well under way even as they made their horrifying predictions—they now hurried to explain it.
It was the roaring 1990s economy, they said, that helped turn back crime.
It was the proliferation of gun control laws, they said.
These theories were not only logical; they were also encouraging, for they attributed the crime drop to specific and recent human initiatives.
If it was gun control and clever police strategies and better-paying jobs that quelled crime—well then, the power to stop criminals had been within our reach all along.
As it would be the next time, God forbid, that crime got so bad.
These theories made their way, seemingly without friction, from the experts’ mouths to journalists’ ears to the public’s mind.
In short course, they became conventional wisdom.
There was another factor, meanwhile, that had greatly contributed to the massive crime drop of the 1990s.
All she had wanted was an abortion.
She was a poor, uneducated, unskilled, alcoholic, drug-using twenty-one-year-old woman who had already given up two children for adoption and now, in 1970, found herself pregnant again.
But in Texas, as in all but a few states at that time, abortion was illegal.
They made her the lead plaintiff in a class-action lawsuit seeking to legalize abortion.
On January 22, 1973, the court ruled in favor of Ms. Roe, allowing legalized abortion throughout the United States.
By this time, of course, it was far too late for Ms. McCorvey/Roe to have her abortion.
She had given birth and put the child up for adoption.
(Years later she would renounce her allegiance to legalized abortion and become a pro-life activist.)
So how did Roe v. Wade help trigger, a generation later, the greatest crime drop in recorded history?
As far as crime is concerned, it turns out that not all children are born equal.
Decades of studies have shown that a child born into an adverse family environment is far more likely than other children to become a criminal.
And the millions of women most likely to have an abortion in the wake of Roe v. Wade—poor, unmarried, and teenage mothers for whom illegal abortions had been too expensive or too hard to get—were often models of adversity.
They were the very women whose children, if born, would have been much more likely than average to become criminals.
But because of Roe v. Wade, these children weren’t being born.
This powerful cause would have a drastic, distant effect: years later, just as these unborn children would have entered their criminal primes, the rate of crime began to plummet.
It wasn’t gun control or a strong economy or new police strategies that finally blunted the American crime wave.
It was, among other factors, the reality that the pool of potential criminals had dramatically shrunk.
Now, as the crime-drop experts (the former crime doomsayers) spun their theories to the media, how many times did they cite legalized abortion as a cause?
It is the quintessential blend of commerce and camaraderie: you hire a real-estate agent to sell your home.
On the sale of a $300,000 house, a typical 6 percent agent fee yields $18,000.
The agent knew how to —what’s that phrase she used?—“maximize the house’s value.” She got you top dollar, right?
A real-estate agent is a different breed of expert than a criminologist, but she is every bit the expert.
You depend on her for this information.
That, in fact, is why you hired an expert.
As the world has grown more specialized, countless such experts have made themselves similarly indispensable.
Doctors, lawyers, contractors, stockbrokers, auto mechanics, mortgage brokers, financial planners: they all enjoy a gigantic informational advantage.
And they use that advantage to help you, the person who hired them, get exactly what you want for the best price.
But experts are human, and humans respond to incentives.
How any given expert treats you, therefore, will depend on how that expert’s incentives are set up.
Sometimes his incentives may work in your favor.
For instance: a study of California auto mechanics found they often passed up a small repair bill by letting failing cars pass emissions inspections—the reason being that lenient mechanics are rewarded with repeat business.
But in a different case, an expert’s incentives may work against you.
In a medical study, it turned out that obstetricians in areas with declining birth rates are much more likely to perform cesarean-section deliveries than obstetricians in growing areas—suggesting that, when business is tough, doctors try to ring up more expensive procedures.
It is one thing to muse about experts’ abusing their position and another to prove it.
The best way to do so would be to measure how an expert treats you versus how he performs the same service for himself.
And real-estate agents often do sell their own homes.
A recent set of data covering the sale of nearly 100,000 houses in suburban Chicago shows that more than 3,000 of those houses were owned by the agents themselves.
Before plunging into the data, it helps to ask a question: what is the real-estate agent’s incentive when she is selling her own home?
And so your incentive and the real-estate agent’s incentive would seem to be nicely aligned.
But as incentives go, commissions are tricky.
First of all, a 6 percent real-estate commission is typically split between the seller ’s agent and the buyer ’s.
Each agent then kicks back roughly half of her take to the agency.
Which means that only 1.5 percent of the purchase price goes directly into your agent’s pocket.
But the agent’s additional share— her personal 1.5 percent of the extra $10,000—is a mere $150.
If you earn $9,400 while she earns only $150, maybe your incentives aren’t aligned after all.
Is the agent willing to put out all that extra time, money, and energy for just $150?
There’s one way to find out: measure the difference between the sales data for houses that belong to real-estate agents themselves and the houses they sold on behalf of clients.
Using the data from the sales of those 100,000 Chicago homes, and controlling for any number of variables—location, age and quality of the house, aesthetics, whether or not the property was an investment, and so on—it turns out that a real-estate agent keeps her own home on the market an average of ten days longer and sells it for an extra 3-plus percent, or $10,000 on a $300,000 house.
When she sells her own house, an agent holds out for the best offer; when she sells yours, she encourages you to take the first decent offer that comes along.
Her share of a better offer—$150—is too puny an incentive to encourage her to do otherwise.
Of all the truisms about politics, one is held to be truer than the rest: money buys elections.
(Disregard for a moment the contrary examples of Steve Forbes, Michael Huffington, and especially Thomas Golisano, who over the course of three gubernatorial elections in New York spent $93 million of his own money and won 4 percent, 8 percent, and 14 percent, respectively, of the vote.)
Most people would agree that money has an undue influence on elections and that far too much money is spent on political campaigns.
It might seem logical to think so, much as it might have seemed logical that a booming 1990s economy helped reduce crime.
Think about this correlation: cities with a lot of murders also tend to have a lot of police officers.
Consider now the police/murder correlation in a pair of real cities.
Denver and Washington, D.C., have about the same population—but Washington has nearly three times as many police as Denver, and it also has eight times the number of murders.
Unless you have more information, however, it’s hard to say what’s causing what.
Someone who didn’t know better might contemplate these figures and conclude that it is all those extra police in Washington who are causing the extra murders.
Consider the folktale of the czar who learned that the most disease-ridden province in his empire was also the province with the most doctors.
He promptly ordered all the doctors shot dead.
Now, returning to the issue of campaign spending: in order to figure out the relationship between money and elections, it helps to consider the incentives at play in campaign finance.
If only Candidate A ran against Candidate B in two consecutive elections but in each case spent different amounts of money.
As it turns out, the same two candidates run against each other in consecutive elections all the time—indeed, in nearly a thousand U.S. congressional races since 1972.
(The same could be said—and will be said, in chapter 5—about parents.)
Some politicians are inherently attractive to voters and others simply aren’t, and no amount of money can do much about it.
In a typical election period that includes campaigns for the presidency, the Senate, and the House of Representatives, about $1 billion is spent per year—which sounds like a lot of money, unless you care to measure it against something seemingly less important than democratic elections.
It is the same amount, for instance, that Americans spend every year on chewing gum.
This isn’t a book about the cost of chewing gum versus campaign spending per se, or about disingenuous real-estate agents, or the impact of legalized abortion on crime.
It will certainly address these scenarios and dozens more, from the art of parenting to the mechanics of cheating, from the inner workings of a crack-selling gang to racial discrimination on The Weakest Link.
We will seek out these answers in the data—whether those data come in the form of schoolchildren’s test scores or New York City’s crime statistics or a crack dealer ’s financial records.
Often we will take advantage of patterns in the data that were incidentally left behind, like an airplane’s sharp contrail in a high sky.
It is well and good to opine or theorize about a subject, as humankind is wont to do, but when moral posturing is replaced by an honest assessment of the data, the result is often a new, surprising insight.
Morality, it could be argued, represents the way that people would like the world to work— whereas economics represents how it actually does work.
Economics is above all a science of measurement.
It comprises an extraordinarily powerful and flexible set of tools that can reliably assess a thicket of information to determine the effect of any one factor, or even the whole effect.
That’s what “the economy” is, after all: a thicket of information about jobs and real estate and banking and investment.
But the tools of economics can be just as easily applied to subjects that are more— well, more interesting.
And understanding them—or, often, ferreting them out—is the key to solving just about any riddle, from violent crime to sports cheating to online dating.
The conventional wisdom is often wrong.
Crime didn’t keep soaring in the 1990s, money alone doesn’t win elections, and—surprise—drinking eight glasses of water a day has never actually been shown to do a thing for your health.
Norma McCorvey had a far greater impact on crime than did the combined forces of gun control, a strong economy, and innovative police strategies.
So did, as we shall see, a man named Oscar Danilo Blandon, aka the Johnny Appleseed of Crack.
“Experts”—from criminologists to real-estate agents—use their informational advantage to serve their own agenda.
And in the face of the Internet, their informational advantage is shrinking every day—as evidenced by, among other things, the falling price of coffins and life-insurance premiums.
Because there is nothing like the sheer power of numbers to scrub away layers of confusion and contradiction.
Most books put forth a single theme, crisply expressed in a sentence or two, and then tell the entire story of that theme: the history of salt; the fragility of democracy; the use and misuse of punctuation.
We did consider, for about six minutes, writing a book that would revolve around a single theme—the theory and practice of applied microeconomics, anyone?—but opted instead for a sort of treasure-hunt approach.
Yes, this approach employs the best analytical tools that economics can offer, but it also allows us to follow whatever freakish curiosities may occur to us.
Thus our invented field of study: Freakonomics.
Since the science of economics is primarily a set of tools, as opposed to a subject matter, then no subject, however offbeat, need be beyond its reach.
It is worth remembering that Adam Smith, the founder of classical economics, was first and foremost a philosopher.
He strove to be a moralist and, in doing so, became an economist.
When he published The Theory of Moral Sentiments in 1759, modern capitalism was just getting under way.
It was the human effect, the fact that economic forces were vastly changing the way a person thought and behaved in a given situation.
In Smith’s era, cause and effect had begun to wildly accelerate; incentives were magnified tenfold.
Smith’s true subject was the friction between individual desire and societal norms.
The economic historian Robert Heilbroner, writing in The Worldly Philosophers, wondered how Smith was able to separate the doings of man, a creature of self-interest, from the greater moral plane in which man operated.
“Smith held that the answer lay in our ability to put ourselves in the position of a third person, an impartial observer,” Heilbroner wrote, “and in this way to form a notion of the objective…merits of a case.” Consider yourself, then, in the company of a third person—or, if you will, a pair of third people —eager to explore the objective merits of interesting cases.
Such as: what do schoolteachers and sumo wrestlers have in common?
1 What Do Schoolteachers and Sumo Wrestlers Have in Common?
Imagine for a moment that you are the manager of a day-care center.
But very often parents are late.
The result: at day’s end, you have some anxious children and at least one teacher who must wait around for the parents to arrive.
A pair of economists who heard of this dilemma—it turned out to be a rather common one— offered a solution: fine the tardy parents.
The economists decided to test their solution by conducting a study of ten day-care centers in Haifa, Israel.
The study lasted twenty weeks, but the fine was not introduced immediately.
For the first four weeks, the economists simply kept track of the number of parents who came late; there were, on average, eight late pickups per week per day-care center.
The fee would be added to the parents’ monthly bill, which was roughly $380.
Before long there were twenty late pickups per week, more than double the original average.
Economics is, at root, the study of incentives: how people get what they want, or need, especially when other people want or need the same thing.
Economists love incentives.
They love to dream them up and enact them, study them and tinker with them.
His solution may not always be pretty—it may involve coercion or exorbitant penalties or the violation of civil liberties—but the original problem, rest assured, will be fixed.
We all learn to respond to incentives, negative and positive, from the outset of life.
But if you make the basketball team, you move up the social ladder.
If you become so excited about your new vice president job that you drive home at eighty mph, you get pulled over by the police and fined $100.
But most incentives don’t come about organically.
There are three basic flavors of incentive: economic, social, and moral.
Very often a single incentive scheme will include all three varieties.
Think about the anti-smoking campaign of recent years.
The addition of a $3-per-pack “sin tax” is a strong economic incentive against buying cigarettes.
The banning of cigarettes in restaurants and bars is a powerful social incentive.
And when the U.S. government asserts that terrorists raise money by selling black-market cigarettes, that acts as a rather jarring moral incentive.
Some of the most compelling incentives yet invented have been put in place to deter crime.
Considering this fact, it might be worthwhile to take a familiar question—why is there so much crime in modern society?—and stand it on its head: why isn’t there a lot more crime?
The chance of going to jail—thereby losing your job, your house, and your freedom, all of which are essentially economic penalties—is certainly a strong incentive.
But when it comes to crime, people also respond to moral incentives (they don’t want to do something they consider wrong) and social incentives (they don’t want to be seen by others as doing something wrong).
For certain types of misbehavior, social incentives are terribly powerful.
In an echo of Hester Prynne’s scarlet letter, many American cities now fight prostitution with a “shaming” offensive, posting pictures of convicted johns (and prostitutes) on websites or on local-access television.
So through a complicated, haphazard, and constantly readjusted web of economic, social, and moral incentives, modern society does its best to militate against crime.
Consider the historical trend in homicide (not including wars), which is both the most reliably measured crime and the best barometer of a society’s overall crime rate.
These statistics, compiled by the criminologist Manuel Eisner, track the historical homicide levels in five European regions.
HOMICIDES (per 100,000 People) The steep decline of these numbers over the centuries suggests that, for one of the gravest human concerns—getting murdered—the incentives that we collectively cook up are working better and better.
So what was wrong with the incentive at the Israeli day-care centers?
That would have likely put an end to the late pickups, though it would have also engendered plenty of ill will.
It substituted an economic incentive (the $3 penalty) for a moral incentive (the guilt that parents were supposed to feel when they came late).
For just a few dollars each day, parents could buy off their guilt.
Furthermore, the small size of the fine sent a signal to the parents that late pickups weren’t such a big problem.
Indeed, when the economists eliminated the $3 fine in the seventeenth week of their study, the number of latearriving parents didn’t change.
Such is the strange and powerful nature of incentives.
Thomas Jefferson noted this while reflecting on the tiny incentive that led to the Boston Tea Party and, in turn, the American Revolution: “So inscrutable is the arrangement of causes and consequences in this world that a two-penny duty on tea, unjustly imposed in a sequestered part of it, changes the condition of all its inhabitants.” In the 1970s, researchers conducted a study that, like the Israeli day-care study, pitted a moral incentive against an economic incentive.
What if the blood donors had been offered an incentive of $50, or $500, or $5,000?
Whatever the incentive, whatever the situation, dishonest people will try to gain an advantage by whatever means necessary.
Or, as W. C. Fields once said: a thing worth having is a thing worth cheating for.
Or the time you really wanted a bagel in the office break room but couldn’t come up with the dollar you were supposed to drop in the coffee can.
And then took the bagel anyway.
Cheating is a primordial economic act: getting more for less.
So it isn’t just the boldface names—inside-trading CEOs and pill-popping ballplayers and perk-abusing politicians— who cheat.
It is the Wal-Mart payroll manager who goes into the computer and shaves his employees’ hours to make his own performance look better.
Some cheating leaves barely a shadow of evidence.
Instead of merely listing the name of each dependent child, tax filers were now required to provide a Social Security number.
Suddenly, seven million children—children who had existed only as phantom exemptions on the previous year ’s 1040 forms—vanished, representing about one in ten of all dependent children in the United States.
The incentive for those cheating taxpayers was quite clear.
The same for the waitress, the payroll manager, and the third grader.
The most volatile current debate among American school administrators, teachers, parents, and students concerns “high-stakes” testing.
The stakes are considered high because instead of simply testing students to measure their progress, schools are increasingly held accountable for the results.
The federal government mandated high-stakes testing as part of the No Child Left Behind law, signed by President Bush in 2002.
Twenty states rewarded individual schools for good test scores or dramatic improvement; thirty-two states sanctioned the schools that didn’t do well.
The Chicago Public School system embraced high-stakes testing in 1996.
The CPS also did away with what is known as social promotion.
Now, in order to be promoted, every student in third, sixth, and eighth grade had to manage a minimum score on the standardized, multiple-choice exam known as the Iowa Test of Basic Skills.
Advocates of high-stakes testing argue that it raises the standards of learning and gives students more incentive to study.
Opponents, meanwhile, worry that certain students will be unfairly penalized if they don’t happen to test well, and that teachers may concentrate on the test topics at the exclusion of more important lessons.
But high-stakes testing has so radically changed the incentives for teachers that they too now have added reason to cheat.
With high-stakes testing, a teacher whose students test poorly can be censured or passed over for a raise or promotion.
High-stakes testing also presents teachers with some positive incentives.
If her students do well enough, she might find herself praised, promoted, and even richer: the state of California at one point introduced bonuses of $25,000 for teachers who produced big test-score gains.
And if a teacher were to survey this newly incentivized landscape and consider somehow inflating her students’ scores, she just might be persuaded by one final incentive: teacher cheating is rarely looked for, hardly ever detected, and just about never punished.
How might a teacher go about cheating?
More broadly, she can “teach to the test,” basing her lesson plans on questions from past years’ exams, which isn’t considered cheating but may well violate the spirit of the test.
Since these tests all have multiple-choice answers, with no penalty for wrong guesses, a teacher might instruct her students to randomly fill in every blank as the clock is winding down, perhaps inserting a long string of Bs or an alternating pattern of Bs and Cs.
If this kind of teacher cheating is truly going on, how might it be detected?
If economics is a science primarily concerned with incentives, it is also—fortunately—a science with statistical tools to measure how people respond to those incentives.
The data also included some information about each teacher and demographic information for every student, as well as his or her past and future test scores—which would prove a key element in detecting the teacher cheating.
What might a cheating teacher ’s classroom look like?
The first thing to search for would be unusual answer patterns in a given classroom: blocks of identical answers, for instance, especially among the harder questions.
One of these classrooms almost certainly had a cheating teacher and the other did not.
Classroom A 112a4a342cb214d0001acd24a3a12dadbcb4a0000000 d4a2341cacbddad3142a2344a2ac23421c00adb4b3cb 1b2a34d4ac42d23b141acd24a3a12dadbcb4a2134141 dbaab3dcacb1dadbc42ac2cc31012dadbcb4adb40000 d12443d43232d32323c213c22d2c23234c332db4b300 db2abad1acbdda212b1acd24a3a12dadbcb400000000 d4aab2124cbddadbcb1a42cca3412dadbcb423134bc1 1b33b4d4a2b1dadbc3ca22c000000000000000000000 d43a3a24acb1d32b412acd24a3a12dadbcb422143bc0 313a3ad1ac3d2a23431223c000012dadbcb400000000 db2a33dcacbd32d313c21142323cc300000000000000 d43ab4d1ac3dd43421240d24a3a12dadbcb400000000 db223a24acb11a3b24cacd12a241cdadbcb4adb4b300 db4abadcacb1dad3141ac212a3a1c3a144ba2db41b43 1142340c2cbddadb4b1acd24a3a12dadbcb43d133bc4 214ab4dc4cbdd31b1b2213c4ad412dadbcb4adb00000 1423b4d4a23d24131413234123a243a2413a21441343 3b3ab4d14c3d2ad4cbcac1c003a12dadbcb4adb40000 dba2ba21ac3d2ad3c4c4cd40a3a12dadbcb400000000 d122ba2cacbd1a13211a2d02a2412d0dbcb4adb4b3c0 144a3adc4cbddadbcbc2c2cc43a12dadbcb4211ab343 d43aba3cacbddadbcbca42c2a3212dadbcb42344b3cb Classroom B db3a431422bd131b4413cd422a1acda332342d3ab4c4 d1aa1a11acb2d3dbc1ca22c23242c3a142b3adb243c1 d42a12d2a4b1d32b21ca2312a3411d00000000000000 3b2a34344c32d21b1123cdc000000000000000000000 34aabad12cbdd3d4c1ca112cad2ccd00000000000000 d33a3431a2b2d2d44b2acd2cad2c2223b40000000000 23aa32d2a1bd2431141342c13d212d233c34a3b3b000 d32234d4a1bdd23b242a22c2a1a1cda2b1baa33a0000 d3aab23c4cbddadb23c322c2a222223232b443b24bc3 d13a14313c31d42b14c421c42332cd2242b3433a3343 d13a3ad122b1da2b11242dc1a3a12100000000000000 d12a3ad1a13d23d3cb2a21ccada24d2131b440000000 314a133c4cbd142141ca424cad34c122413223ba4b40 d42a3adcacbddadbc42ac2c2ada2cda341baa3b24321 db1134dc2cb2dadb24c412c1ada2c3a341ba20000000 d1341431acbddad3c4c213412da22d3d1132a1344b1b 1ba41a21a1b2dadb24ca22c1ada2cd32413200000000 dbaa33d2a2bddadbcbca11c2a2accda1b2ba20000000 If you guessed that classroom A was the cheating classroom, congratulations.
Here again are the answer strings from classroom A, now reordered by a computer that has been asked to apply the cheating algorithm and seek out suspicious patterns.
Classroom A (With cheating algorithm applied) 1.
Did fifteen out of twenty-two students somehow manage to reel off the same six consecutive correct answers (the d-a-d-b-c-b string) all by themselves?
Why on earth would a cheating teacher go to the trouble of erasing a student’s test sheet and then fill in the wrong answer?
Another indication of teacher cheating in classroom A is the class’s overall performance.
As sixth graders who were taking the test in the eighth month of the academic year, these students needed to achieve an average score of 6.8 to be considered up to national standards.
The students in classroom A averaged 5.8 on their sixth-grade tests, which is a full grade level below where they should be.
When these sixth-grade students reached seventh grade, they averaged 5.5—more than two grade levels below standard and even worse than they did in sixth grade.
So an entire roomful of children in classroom A suddenly got very smart one year and very dim the next, or more likely, their sixth-grade teacher worked some magic with her pencil.
There are two noteworthy points to be made about the children in classroom A, tangential to the cheating itself.
The first is that they are obviously in poor academic shape, which makes them the very children whom high-stakes testing is promoted as helping the most.
The second point is that these students (and their parents) would be in for a terrible shock once they reached the seventh grade.
This may be the cruelest twist yet in highstakes testing.
A cheating teacher may tell herself that she is helping her students, but the fact is that she would appear far more concerned with helping herself.
An analysis of the entire Chicago data reveals evidence of teacher cheating in more than two hundred classrooms per year, roughly 5 percent of the total.
This is a conservative estimate, since the algorithm was able to identify only the most egregious form of cheating—in which teachers systematically changed students’ answers—and not the many subtler ways a teacher might cheat.
In a recent study among North Carolina schoolteachers, some 35 percent of the respondents said they had witnessed their colleagues cheating in some fashion, whether by giving students extra time, suggesting answers, or manually changing students’ answers.
What are the characteristics of a cheating teacher?
The Chicago data shows that male and female teachers are equally prone to cheating.
A cheating teacher tends to be younger and less qualified than average.
She is also more likely to cheat after her incentives change.
Because the Chicago data ran from 1993 to 2000, it bracketed the introduction of high-stakes testing in 1996.
Sure enough, there was a pronounced spike in cheating in 1996.
Nor was the cheating random.
It was the teachers in the lowest-scoring classrooms who were most likely to cheat.
It should also be noted that the $25,000 bonus for California teachers was eventually revoked, in part because of suspicions that too much of the money was going to cheaters.
Not every result of the Chicago cheating analysis was so dour.
In addition to detecting cheaters, the algorithm could also identify the best teachers in the school system.
But in early 2002, the new CEO of the Chicago Public Schools, Arne Duncan, contacted the study’s authors.
Rather, he wanted to make sure that the teachers identified by the algorithm as cheaters were truly cheating—and then do something about it.
He was only thirty-six when appointed, a onetime academic all-American at Harvard who later played pro basketball in Australia.
He had spent just three years with the CPS—and never in a job important enough to have his own secretary—before becoming its CEO.
When Duncan was a boy, his afterschool playmates were the underprivileged kids his mother cared for.
So when he took over the public schools, his allegiance lay more with schoolchildren and their families than with teachers and their union.
The best way to get rid of cheating teachers, Duncan had decided, was to readminister the standardized exam.
He only had the resources to retest 120 classrooms, however, so he asked the creators of the cheating algorithm to help choose which classrooms to test.
It might have seemed sensible to retest only the classrooms that likely had a cheating teacher.
But even if their retest scores were lower, the teachers could argue that the students did worse merely because they were told that the scores wouldn’t count in their official record—which, in fact, all retested students would be told.
The classrooms shown by the algorithm to have the best teachers, in which big gains were thought to have been legitimately attained.
If those classrooms held their gains while the classrooms with a suspected cheater lost ground, the cheating teachers could hardly argue that their students did worse only because the scores wouldn’t count.
More than half of the 120 retested classrooms were those suspected of having a cheating teacher.
The remainder were divided between the supposedly excellent teachers (high scores but no suspicious answer patterns) and, as a further control, classrooms with mediocre scores and no suspicious answers.
Neither were the teachers.
But they may have gotten the idea when it was announced that CPS officials, not the teachers, would administer the test.
The teachers were asked to stay in the classroom with their students, but they would not be allowed to even touch the answer sheets.
The results were as compelling as the cheating algorithm had predicted.
In the classrooms chosen as controls, where no cheating was suspected, scores stayed about the same or even rose.
In contrast, the students with the teachers identified as cheaters scored far worse, by an average of more than a full grade level.
As a result, the Chicago Public School system began to fire its cheating teachers.
The final outcome of the Chicago study is further testament to the power of incentives: the following year, cheating by teachers fell more than 30 percent.
You might think that the sophistication of teachers who cheat would increase along with the level of schooling.
The course was called Coaching Principles and Strategies of Basketball, and the final grade was based on a single exam that had twenty questions.
Among the questions: How many halves are in a college basketball game?
field goal account for in a basketball game?
a. Ron Jirsa b. John Pelphrey c. Jim Harrick Jr. d. Steve Wojciechowski If you are stumped by the final question, it might help to know that Coaching Principles was taught by Jim Harrick Jr., an assistant coach with the university’s basketball team.
It might also help to know that his father, Jim Harrick Sr., was the head basketball coach.
If it strikes you as disgraceful that Chicago schoolteachers and University of Georgia professors will cheat—a teacher, after all, is meant to instill values along with the facts—then the thought of cheating among sumo wrestlers may also be deeply disturbing.
With its purification rituals and its imperial roots, sumo is sacrosanct in a way that American sports will never be.
It is true that sports and cheating go hand in hand.
That’s because cheating is more common in the face of a bright-line incentive (the line between winning and losing, for instance) than with a murky incentive.
Olympic sprinters and weightlifters, cyclists in the Tour de France, football linemen and baseball sluggers: they have all been shown to swallow whatever pill or powder may give them an edge.
Cagey baseball managers try to steal an opponent’s signs.
(The man accused of orchestrating the vote swap, a reputed Russian mob boss named Alimzhan Tokhtakhounov, was also suspected of rigging beauty pageants in Moscow.)
An athlete who gets caught cheating is generally condemned, but most fans at least appreciate his motive: he wanted so badly to win that he bent the rules.
(As the baseball player Mark Grace once said, “If you’re not cheating, you’re not trying.”) An athlete who cheats to lose, meanwhile, is consigned to a deep circle of sporting hell.
The 1919 Chicago White Sox, who conspired with gamblers to throw the World Series (and are therefore known forever as the Black Sox), retain a stench of iniquity among even casual baseball fans.
The City College of New York’s championship basketball team, once beloved for its smart and scrappy play, was instantly reviled when it was discovered in 1951 that several players had taken mob money to shave points—intentionally missing baskets to help gamblers beat the point spread.
Remember Terry Malloy, the tormented former boxer played by Marlon Brando in On the Waterfront?
If cheating to lose is sport’s premier sin, and if sumo wrestling is the premier sport of a great nation, cheating to lose couldn’t possibly exist in sumo.
Each wrestler maintains a ranking that affects every slice of his life: how much money he makes, how large an entourage he carries, how much he gets to eat, sleep, and otherwise take advantage of his success.
So a wrestler entering the final day of a tournament on the bubble, with a 7–7 record, has far more to gain from a victory than an opponent with a record of 8–6 has to lose.
A sumo bout is a concentrated flurry of force and speed and leverage, often lasting only a few seconds.
Let’s imagine for a moment that sumo wrestling is rigged.
Wrestlers on the bubble also do astonishingly well against 9–5 opponents: As suspicious as this looks, a high winning percentage alone isn’t enough to prove that a match is rigged.
Furthermore, each wrestler belongs to a stable that is typically managed by a former sumo champion, so even the rival stables have close ties.
Now let’s look at the win-loss percentage between the 7–7 wrestlers and the 8–6 wrestlers the next time they meet, when neither one is on the bubble.
It’s especially interesting to note that by the two wrestlers’ second subsequent meeting, the win percentages revert to the expected level of about 50 percent, suggesting that the collusion spans only two matches.
Officials from the Japanese Sumo Association typically dismiss any such charges as fabrications by disgruntled former wrestlers.
Still, allegations of match rigging do occasionally find their way into the Japanese media.
These occasional media storms offer one more chance to measure possible corruption in sumo.
The data show that in the sumo tournaments held immediately after allegations of match rigging, 7–7 wrestlers win only 50 percent of their final-day matches against 8–6 opponents instead of the typical 80 percent.
No matter how the data are sliced, they inevitably suggest one thing: it is hard to argue that sumo wrestling isn’t rigged.
Aside from the crooked matches, they said, sumo was rife with drug use and sexcapades, bribes and tax evasion, and close ties to the yakuza, the Japanese mafia.
The two men began to receive threatening phone calls; one of them told friends he was afraid he would be killed by the yakuza.
The police declared there had been no foul play but did not conduct an investigation.
“But no one has seen them poisoned, so you can’t prove the skepticism.” Whether or not their deaths were intentional, these two men had done what no other sumo insider had previously done: named names.
So if sumo wrestlers, schoolteachers, and day-care parents all cheat, are we to assume that mankind is innately and universally corrupt?
The answer may lie in…bagels.
With early training in agricultural economics, he wanted to tackle world hunger.
He held senior-level jobs and earned good money, but he wasn’t always recognized for his best work.
At the office Christmas party, colleagues would introduce him to their wives not as “the head of the public research group” (which he was) but as “the guy who brings in the bagels.” The bagels had begun as a casual gesture: a boss treating his employees whenever they won a research contract.
Every Friday, he would bring in some bagels, a serrated knife, and cream cheese.
When employees from neighboring floors heard about the bagels, they wanted some too.
Eventually he was bringing in fifteen dozen bagels a week.
In 1984, when his research institute fell under new management, Feldman took a look at his future and grimaced.
He decided to quit his job and sell bagels.
The last of their three children was finishing college, and they had retired their mortgage.
Driving around the office parks that encircle Washington, he solicited customers with a simple pitch: early in the morning, he would deliver some bagels and a cash basket to a company’s snack room; he would return before lunch to pick up the money and the leftovers.
Within a few years, Feldman was delivering 8,400 bagels a week to 140 companies and earning as much as he had ever made as a research analyst.
He had also—quite without meaning to—designed a beautiful economic experiment.
From the beginning, Feldman kept rigorous data on his bagel business.
So by measuring the money collected against the bagels taken, he found it possible to tell, down to the penny, just how honest his customers were.
As it happens, Feldman’s accidental study provides a window onto a form of cheating that has long stymied academics: white-collar crime.
(Yes, shorting the bagel man is white-collar crime, writ however small.)
It might seem ludicrous to address as large and intractable a problem as white-collar crime through the life of a bagel man.
Despite all the attention paid to rogue companies like Enron, academics know very little about the practicalities of white-collar crime.
A key fact of whitecollar crime is that we hear about only the very slim fraction of people who are caught cheating.
Most embezzlers lead quiet and theoretically happy lives; employees who steal company property are rarely detected.
With street crime, meanwhile, that is not the case.
A mugging or a burglary or a murder is usually tallied whether or not the criminal is caught.
A street crime has a victim, who typically reports the crime to the police, who generate data, which in turn generate thousands of academic papers by criminologists, sociologists, and economists.
But white-collar crime presents no obvious victim.
From whom, exactly, did the masters of Enron steal?
Paul Feldman’s bagel business was different.
But just as crime tends to be low on a street where a police car is parked, the 95 percent rate was artificially high: Feldman’s presence had deterred theft.
Not only that, but those bagel eaters knew the provider and had feelings (presumably good ones) about him.
A broad swath of psychological and economic research has shown that people will pay different amounts for the same item depending on who is providing it.
The economist Richard Thaler, in his 1985 “Beer on the Beach” study, showed that a thirsty sunbather would pay $2.65 for a beer delivered from a resort hotel but only $1.50 for the same beer if it came from a shabby grocery store.
He considered a rate between 80 and 90 percent “annoying but tolerable.” If a company habitually paid below 80 percent, Feldman might post a hectoring note, like this one: The cost of bagels has gone up dramatically since the beginning of the year.
Unfortunately, the number of bagels that disappear without being paid for has also gone up.
Each year he drops off about seven thousand boxes and loses, on average, just one to theft.
This is an intriguing statistic: the same people who routinely steal more than 10 percent of his bagels almost never stoop to stealing his money box—a tribute to the nuanced social calculus of theft.
From Feldman’s perspective, an office worker who eats a bagel without paying is committing a crime; the office worker probably doesn’t think so.
This distinction probably has less to do with the admittedly small amount of money involved (Feldman’s bagels cost one dollar each, cream cheese included) than with the context of the “crime.” The same office worker who fails to pay for his bagel might also help himself to a long slurp of soda while filling a glass in a self-serve restaurant, but he is very unlikely to leave the restaurant without paying.
So what do the bagel data have to say?
But immediately after September 11 of that year, the rate spiked a full 2 percent and hasn’t slipped much since.
(If a 2 percent gain in payment doesn’t sound like much, think of it this way: the nonpayment rate fell from 13 to 11 percent, which amounts to a 15 percent decline in theft.)
Because many of Feldman’s customers are affiliated with national security, there may have been a patriotic element to this 9/11 Effect.
In a bigger office, a bigger crowd is bound to convene around the bagel table, providing more witnesses to make sure you drop your money in the box.
But in the bigoffice/small-office comparison, bagel crime seems to mirror street crime.
There is far less street crime per capita in rural areas than in cities, in large part because a rural criminal is more likely to be known (and therefore caught).
Also, a smaller community tends to exert greater social incentives against crime, the main one being shame.
The bagel data also reflect how much personal mood seems to affect honesty.
Unseasonably pleasant weather inspires people to pay at a higher rate.
Unseasonably cold weather, meanwhile, makes people cheat prolifically; so do heavy rain and wind.
The week of Christmas produces a 2 percent drop in payment rates—again, a 15 percent increase in theft, an effect on the same magnitude, in reverse, as that of 9/11.
The low-cheating holidays represent little more than an extra day off from work.
The high-cheating holidays are fraught with miscellaneous anxieties and the high expectations of loved ones.
Feldman has also reached some of his own conclusions about honesty, based more on his experience than the data.
He has come to believe that morale is a big factor—that an office is more honest when the employees like their boss and their work.
He also believes that employees further up the corporate ladder cheat more than those down below.
What he didn’t consider is that perhaps cheating was how they got to be executives.)
If morality represents the way we would like the world to work and economics represents how it actually does work, then the story of Feldman’s bagel business lies at the very intersection of morality and economics.
In fact, the theme of Smith’s first book, The Theory of Moral Sentiments, was the innate honesty of mankind.
It comes from Plato’s Republic.
A student named Glaucon offered the story in response to a lesson by Socrates—who, like Adam Smith, argued that people are generally good even without enforcement.
He told of a shepherd named Gyges who stumbled upon a secret cavern with a corpse inside that wore a ring.
Glaucon’s story posed a moral question: could any man resist the temptation of evil if he knew his acts could not be witnessed?
But Paul Feldman sides with Socrates and Adam Smith— for he knows that the answer, at least 87 percent of the time, is yes.
2 How Is the Ku Klux Klan Like a Group of Real-Estate Agents?
As institutions go, the Ku Klux Klan has had a markedly up-and-down history.
It was founded in the immediate aftermath of the Civil War by six former Confederate soldiers in Pulaski, Tennessee.
Thus the name they chose, “kuklux,” a slight mangling of kuklos, the Greek word for “circle.” In the beginning, their activities were said to be harmless midnight pranks—for instance, riding horses through the countryside while draped in white sheets and pillowcase hoods.
Among its regional leaders were five former Confederate generals; its staunchest supporters were the plantation owners for whom Reconstruction posed an economic and political nightmare.
In 1872, President Ulysses S. Grant spelled out for the House of Representatives the true aims of the Ku Klux Klan: “By force and terror, to prevent all political action not in accord with the views of the members, to deprive colored citizens of the right to bear arms and of the right of a free ballot, to suppress the schools in which colored children were taught, and to reduce the colored people to a condition closely allied to that of slavery.” The early Klan did its work through pamphleteering, lynching, shooting, burning, castrating, pistol-whipping, and a thousand forms of intimidation.
They targeted former slaves and any whites who supported the blacks’ rights to vote, acquire land, or gain an education.
If the Klan itself was defeated, however, its aims had largely been achieved through the establishment of Jim Crow laws.
Congress, which during Reconstruction had been quick to enact measures of legal, social, and economic freedom for blacks, just as quickly began to roll them back.
In Plessy v. Ferguson, the U.S. Supreme Court gave the go-ahead to fullscale racial segregation.
The Ku Klux Klan lay largely dormant until 1915, when D. W. Griffith’s film The Birth of a Nation (originally titled The Clansman) helped spark its rebirth.
The film quoted a line from A History of the American People, written by a renowned historian: “At last there had sprung into existence a great Ku Klux Klan, a veritable empire of the South, to protect the Southern country.” The historian in question was U.S. president Woodrow Wilson, onetime scholar and president of Princeton University.
“Looks to me like it’s the Ku Klux that he is copying.” The onset of World War II and a number of internal scandals once again laid the Klan low.
Public sentiment turned against the Klan as the unity of a country at war trumped its message of separatism.
The Klan was thought to hold great sway with key Georgia politicians, and its Georgia chapters were said to include many policemen and sheriff’s deputies.
Yes, the Klan was a secret society, reveling in passwords and cloak-and-dagger ploys, but its real power lay in the very public fear that it fostered, exemplified by the open secret that the Ku Klux Klan and the law-enforcement establishment were brothers in arms.
He came from a good southern family which claimed ancestors including two signers of the Declaration of Independence, an officer in the Confederate Army, and John B. Stetson, founder of the famed hat company and the man for whom Stetson University was named.
Years later, when he served as a rare white correspondent for the Pittsburgh Courier, the country’s largest black newspaper, he wrote under the pseudonym Daddy Mention—after a black folk hero who, as myth told it, could outrun the blast of a sheriff’s shotgun.
What drove Kennedy was a hatred of small-mindedness, ignorance, obstructionism, and intimidation—which, in his view, were displayed by no organization more proudly than the Ku Klux Klan.
And the few anti-hate groups that existed at the time had little leverage or even information about the Klan.
He would spend years interviewing Klan leaders and sympathizers, sometimes taking advantage of his own background and lineage to pretend that he was on their side of the issues.
Regardless, there was a great deal of information to be gleaned from this Brown/Kennedy collaboration.
Brown divulged what he was learning at the weekly Klan meetings: the identities of the Klan’s local and regional leaders; their upcoming plans; the Klan’s current rituals, passwords, and language.
The secret Klan handshake was a left-handed, limpwristed fish wiggle.
Ayak”—“Ayak” being code for “Are You a Klansman?” He would hope to hear this response: “Yes, and I also know a Mr. Akai”—code for “A Klansman Am I.” Before long, John Brown was invited to join the Klavaliers, the Klan’s secret police and “flog squad.” For an infiltrator, this posed a particularly sticky problem: What would happen if he were called upon to inflict violence?
But as it happened, a central tenet of life in the Klan—and of terrorism in general—is that most of the threatened violence never goes beyond the threat stage.
Consider lynching, the Klan’s hallmark sign of violence.
Here, compiled by the Tuskegee Institute, are the decade-by-decade statistics on the lynching of blacks in the United States: Bear in mind that these figures represent not only lynchings attributed to the Ku Klux Klan but the total number of reported lynchings.
The first is the obvious decrease in lynchings over time.
The second is the absence of a correlation between lynchings and Klan membership: there were actually more lynchings of blacks between 1900 and 1909, when the Klan was dormant, than during the 1920s, when the Klan had millions of members— which suggests that the Ku Klux Klan carried out far fewer lynchings than is generally thought.
Third, relative to the size of the black population, lynchings were exceedingly rare.
To be sure, one lynching is one too many.
But by the turn of the century, lynchings were hardly the everyday occurrence that they are often considered in the public recollection.
Compare the 281 victims of lynchings in the 1920s to the number of black infants who were dying at that time as a result of malnutrition, pneumonia, diarrhea, and the like.
What larger truths do these lynching figures suggest?
What does it mean that lynchings were relatively rare and that they fell precipitously over time, even in the face of a boom in Klan membership?
The most compelling explanation is that all those early lynchings worked.
White racists— whether or not they belonged to the Ku Klux Klan—had through their actions and their rhetoric developed a strong incentive scheme that was terribly clear and terribly frightening.
One or two lynchings went a long way toward inducing docility among even a large group of people, for people respond strongly to strong incentives.
And there are few incentives more powerful than the fear of random violence—which, in essence, is why terrorism is so effective.
But if the Ku Klux Klan of the 1940s wasn’t uniformly violent, what was it?
That their fraternity engaged in quasi-religious chanting and oath taking and hosanna hailing, all of it top secret, made it that much more appealing.
Then there were rackets like the Klan’s Death Benefit Association, which sold insurance policies to Klan members and accepted only cash or personal checks made out to the Grand Dragon himself.
And, even though the Klan may not have been as deadly as generally thought, it was plenty violent and, perhaps worse, had ever greater designs on political influence.
Kennedy was therefore eager to damage the Klan in any way he could.
When he heard about Klan plans for a union-busting rally, he fed the information to a union friend.
He passed along Klan information to the assistant attorney general of Georgia, an established Klan buster.
After researching the Klan’s corporate charter, Kennedy wrote to the governor of Georgia suggesting the grounds upon which the charter should be revoked: the Klan had been designated a non-profit, non-political organization, but Kennedy had proof that it was clearly devoted to both profits and politics.
And even if he could somehow damage the Klan in Atlanta, the hundreds of other chapters around the country would go untouched.
He had noticed one day a group of young boys playing some kind of spy game in which they exchanged silly secret passwords.
Wouldn’t it be nice, he thought, to get the Klan’s passwords and the rest of its secrets into the hands of kids all across the country—and their parents too?
What better way to defang a secret society than to make public its most secret information?
Instead of futilely attacking the Klan from the outside, what if he could somehow unleash all the secret inside information that John Brown was gathering from the Klan’s weekly meetings?
Between Brown’s inside dope and everything that Kennedy had learned via his own investigations, he probably knew more Klan secrets than the average Klansman.
He began feeding Klan reports to the journalist Drew Pearson, whose Washington Merry-Go-Round program was heard by millions of adults every day, and to the producers of the Adventures of Superman show, which reached millions of children each night.
He told them about Mr. Ayak and Mr. Akai, and he passed along overheated passages from the Klan’s bible, which was called the Kloran.
He explained the role of Klan officers in any local Klavern: the Klaliff (vice president), Klokard (lecturer), Kludd (chaplain), Kligrapp (secretary), Klabee (treasurer), Kladd (conductor), Klarogo (inner guard), Klexter (outer guard), the Klokann (a five-man investigative committee), and the Klavaliers (whose leader was called Chief Ass Tearer).
And Kennedy passed along all the information and gossip that John Brown gleaned by infiltrating the main Klan chapter, Nathan Bedford Forrest Klavern No.
During the war, the Adventures of Superman program had portrayed its hero fighting Hitler and Mussolini and Hirohito.
1, Atlanta, Ga., the week after elections, the Grand Dragon wrung his hands and once again cautioned Klansmen to be careful about leaks.
“I have to talk frankly at these meetings,” he said, “but I might as well call Drew Pearson before I come to the meeting and give him the information, for [the] next day he gives it out to everybody from coast to coast.
He added that the Klavalier Klub—the Klan’s whipping and flogging department—was now on the job and had plenty of friends on the Atlanta police force.
As the Pearson and Superman radio shows played on, and as Stetson Kennedy continued to relay the Klan secrets obtained by John Brown to other broadcast and print outlets, a funny thing happened: attendance at Klan meetings began to fall, as did applications for new membership.
He turned the Klan’s secrecy against itself by making its private information public; he converted heretofore precious knowledge into ammunition for mockery.
Americans who might have been philosophically inclined to oppose the Klan had now been given enough specific information to oppose them more actively, and public sentiment began to shift.
Although the Klan would never quite die, especially down south— David Duke, a smooth-talking Klan leader from Louisiana, mounted substantive bids for the U.S. Senate and other offices—it was certainly handicapped, at least in the short term, by Kennedy’s brazen dissemination of inside information.
While it is impossible to tease out the exact impact that his work had on the Klan, many people have given him a great deal of credit for damaging an institution that was in grave need of being damaged.
This did not come about because Stetson Kennedy was courageous or resolute or unflappable, even though he was all of these.
It happened because he understood the raw power of information.
The Ku Klux Klan—much like politicians or real-estate agents or stockbrokers—was a group whose power was derived in large part from the fact that it hoarded information.
Once that information falls into the wrong hands (or, depending on your point of view, the right hands), much of the group’s advantage disappears.
Other types of insurance, including health and automobile and homeowners’ coverage, were certainly not falling in price.
The Internet happened.
Like Stetson Kennedy, they were dealing in information.
(Had the Internet been around when Kennedy was attacking the Klan, he probably would have been blogging his brains out.)
To be sure, there are differences between exposing the Ku Klux Klan and exposing insurance companies’ high premiums.
The Klan trafficked in secret information whose secrecy engendered fear, while insurance prices were less a secret than a set of facts dispensed in a way that made comparisons difficult.
But in both instances, the dissemination of the information diluted its power.
Information is so powerful that the assumption of information, even if the information does not actually exist, can have a sobering effect.
Because the only person who might logically want to resell a brand-new car is someone who found the car to be a lemon.
He assumes that the seller has some information about the car that he, the buyer, does not have—and the seller is punished for this assumed information.
It is common for one party to a transaction to have better information than another party.
In the parlance of economists, such a case is known as an information asymmetry.
We accept as a verity of capitalism that someone (usually an expert) knows more than someone else (usually a consumer).
But information asymmetries everywhere have in fact been gravely wounded by the Internet.
Information is the currency of the Internet.
As a medium, the Internet is brilliantly efficient at shifting information from the hands of those who have it into the hands of those who do not.
Often, as in the case of term life insurance prices, the information existed but in a woefully scattered way.
(In such instances, the Internet acts like a gigantic horseshoe magnet waved over an endless sea of haystacks, plucking the needle out of each one.)
The Internet has accomplished what even the most fervent consumer advocates usually cannot: it has vastly shrunk the gap between the experts and the public.
The Internet has proven particularly fruitful for situations in which a face-to-face encounter with an expert might actually exacerbate the problem of asymmetrical information—situations in which an expert uses his informational advantage to make us feel stupid or rushed or cheap or ignoble.
Or consider the automobile dealership: a salesman does his best to obscure the car ’s base price under a mountain of add-ons and incentives.
Later, however, in the coolheaded calm of your home, you can use the Internet to find out exactly how much the dealer paid the manufacturer for that car.
The Internet, powerful as it is, has hardly slain the beast that is information asymmetry.
Consider the so-called corporate scandals of the early 2000s.
The crimes committed by Enron included hidden partnerships, disguised debt, and the manipulation of energy markets.
Henry Blodget of Merrill Lynch and Jack Grubman of Salomon Smith Barney wrote glowing research reports of companies they knew to be junk.
Sam Waksal dumped his ImClone stock when he got early word of a damaging report from the Food and Drug Administration; his friend Martha Stewart also dumped her shares, then lied about the reason.
WorldCom and Global Crossing fabricated billions of dollars in revenues to pump up their stock prices.
One group of mutual fund companies let preferred customers trade at preferred prices, and another group was charged with hiding management fees.
Though extraordinarily diverse, these crimes all have a common trait: they were sins of information.
Most of them involved an expert, or a gang of experts, promoting false information or hiding true information; in each case the experts were trying to keep the information asymmetry as asymmetrical as possible.
One characteristic of information crimes is that very few of them are detected.
Unlike street crimes, they do not leave behind a corpse or a broken window.
Unlike a bagel criminal—that is, someone who eats one of Paul Feldman’s bagels but doesn’t pay—an information criminal typically doesn’t have someone like Feldman tallying every nickel.
For an information crime to reach the surface, something drastic must happen.
Consider the “Enron tapes,” the secretly recorded conversations of Enron employees that surfaced after the company imploded.
During a phone conversation on August 5, 2000, two traders chatted about how a wildfire in California would allow Enron to jack up its electricity prices.
“The magical word of the day,” one trader said, “is ‘Burn, Baby, Burn.’” A few months later, a pair of Enron traders named Kevin and Bob talked about how California officials wanted to make Enron refund the profits of its price gouging.
KEVIN If you were to assume that many experts use their information to your detriment, you’d be right.
Experts depend on the fact that you don’t have the information they do.
Or that you are so befuddled by the complexity of their operation that you wouldn’t know what to do with the information if you had it.
Or that you are so in awe of their expertise that you wouldn’t dare challenge them.
If your doctor suggests that you have angioplasty—even though some current research suggests that angioplasty often does little to prevent heart attacks—you aren’t likely to think that the doctor is using his informational advantage to make a few thousand dollars for himself or his buddy.
But as David Hillis, an interventional cardiologist at the University of Texas Southwestern Medical Center in Dallas, explained to the New York Times, a doctor may have the same economic incentives as a car salesman or a funeral director or a mutual fund manager: “If you’re an invasive cardiologist and Joe Smith, the local internist, is sending you patients, and if you tell them they don’t need the procedure, pretty soon Joe Smith doesn’t send patients anymore.” Armed with information, experts can exert a gigantic, if unspoken, leverage: fear.
Fear that your children will find you dead on the bathroom floor of a heart attack if you do not have angioplasty surgery.
The fear created by commercial experts may not quite rival the fear created by terrorists like the Ku Klux Klan, but the principle is the same.
It is the job of your real-estate agent, of course, to find the golden mean.
She is the one with all the information: the inventory of similar houses, the recent sales trends, the tremors of the mortgage market, perhaps even a lead on an interested buyer.
You feel fortunate to have such a knowledgeable expert as an ally in this most confounding enterprise.
A real-estate agent may see you not so much as an ally but as a mark.
Think back to the study cited at the beginning of this book, which measured the difference between the sale prices of homes that belonged to real-estate agents themselves and the houses they sold for their clients.
The study found that an agent keeps her own house on the market an average ten extra days, waiting for a better offer, and sells it for over 3 percent more than your house—or $10,000 on the sale of a $300,000 house.
That’s $10,000 going into her pocket that does not go into yours, a nifty profit produced by the abuse of information and a keen understanding of incentives.
The problem is that the agent only stands to personally gain an additional $150 by selling your house for $10,000 more, which isn’t much reward for a lot of extra work.
The agent does not want to come right out and call you a fool.
Here is the agent’s main weapon: the conversion of information into fear.
Consider this true story, related by John Donohue, a law professor who in 2001 was teaching at Stanford University: “I was just about to buy a house on the Stanford campus,” he recalls, “and the seller ’s agent kept telling me what a good deal I was getting because the market was about to zoom.
As soon as I signed the purchase contract, he asked me if I would need an agent to sell my previous Stanford house.
I told him that I would probably try to sell without an agent, and he replied, ‘John, that might work under normal conditions, but with the market tanking now, you really need the help of a broker.’” Within five minutes, a zooming market had tanked.
Such are the marvels that can be conjured by an agent in search of the next deal.
Consider now another true story of a real-estate agent’s information abuse.
He was prepared to offer $450,000 but he first called the seller ’s agent and asked her to name the lowest price that she thought the homeowner might accept.
The agent promptly scolded K. “You ought to be ashamed of yourself,” she said.
After ten minutes, as the conversation was ending, the agent told K., “Let me say one last thing.
Thanks to his own agent’s intervention, the seller lost at least $20,000.
The agent, meanwhile, only lost $300—a small price to pay to ensure that she would quickly and easily lock up the sale, which netted her a commission of $6,450.
So a big part of a real-estate agent’s job, it would seem, is to persuade the homeowner to sell for less than he would like while at the same time letting potential buyers know that a house can be bought for less than its listing price.
The study of real-estate agents cited above also includes data that reveals how agents convey information through the for-sale ads they write.
A phrase like “well maintained,” for instance, is as full of meaning to an agent as “Mr.
A savvy buyer will know this (or find out for himself once he sees the house), but to the sixty-five-year-old retiree who is selling his house, “well maintained” might sound like a compliment, which is just what the agent intends.
An analysis of the language used in real-estate ads shows that certain words are powerfully correlated with the final sale price of a house.
It does, however, indicate that when a real-estate agent labels a house “well maintained,” she may be subtly encouraging a buyer to bid low.
As information goes, such terms are specific and straightforward— and therefore pretty useful.
“Fantastic,” meanwhile, is a dangerously ambiguous adjective, as is “charming.” Both these words seem to be real-estate agent code for a house that doesn’t have many specific attributes worth describing.
And an exclamation point in a real-estate ad is bad news for sure, a bid to paper over real shortcomings with false enthusiasm.
If you study the words in ads for a real-estate agent’s own home, meanwhile, you see that she indeed emphasizes descriptive terms (especially “new,” “granite,” “maple,” and “move-in condition”) and avoids empty adjectives (including “wonderful,” “immaculate,” and the telltale “!”).
She is careful to exercise every advantage of the information asymmetry she enjoys.
The point here is not that real-estate agents are bad people, but that they simply are people—and people inevitably respond to incentives.
The incentives of the real-estate business, as currently configured, plainly encourage some agents to act against the best interests of their customers.
But like the funeral director and the car salesman and the life-insurance company, the real-estate agent has also seen her advantage eroded by the Internet.
After all, anyone selling a home can now get online and gather her own information about sales trends and housing inventory and mortgage rates.
The information has been set loose.
Real-estate agents still get a higher price for their own homes than comparable homes owned by their clients, but since the proliferation of real-estate websites, the gap between the two prices has shrunk by a third.
It would be naïve to suppose that people abuse information only when they are acting as experts or as agents of commerce.
After all, agents and experts are people too—which suggests that we are likely to abuse information in our personal lives as well, whether by withholding true information or editing the information we choose to put forth.
A real-estate agent may wink and nod when she lists a “well-maintained” house, but we each have our equivalent hedges.
(For even more fun, compare that first-date conversation to a conversation with the same person during your tenth year of marriage.)
Or think about how you might present yourself if you were going on national television for the first time.
What sort of image would you want to project?
During the heyday of the Ku Klux Klan, its members took pride in publicly disparaging anybody who wasn’t a conservative white Christian.
Might there be a way to test for discrimination in a public setting?
Unlikely as it may seem, the television game show The Weakest Link provides a unique laboratory to study discrimination.
The game includes eight contestants (or, in a later daytime version, six) who each answer trivia questions and compete for a single cash jackpot.
A player ’s trivia-answering ability is presumably the only worthwhile factor to consider; race, gender, and age wouldn’t seem to matter.
By measuring a contestant’s actual votes against the votes that would truly best serve his self-interest, it’s possible to tell if discrimination is at play.
The voting strategy changes as the game progresses.
In later rounds, the strategic incentives are flipped.
The key to measuring the Weakest Link voting data is to tease out a contestant’s playing ability from his race, gender, and age.
If a young black man answers a lot of questions correctly but is voted off early, discrimination would seem to be a factor.
Meanwhile, if an elderly white woman doesn’t answer a single question correctly and is still not voted off, some sort of discriminatory favoritism would seem to be at play.
Two of the most potent social campaigns of the past half-century were the civil rights movement and the feminist movement, which demonized discrimination against blacks and women, respectively.
So perhaps, you say hopefully, discrimination was practically eradicated during the twentieth century, like polio.
Or more likely, it has become so unfashionable to discriminate against certain groups that all but the most insensitive people take pains to at least appear fair-minded, at least in public.
This hardly means that discrimination itself has ended—only that people are embarrassed to show it.
How might you determine whether the lack of discrimination against blacks and women represents a true absence or just a charade?
Indeed, the Weakest Link voting data do indicate two kinds of contestants who are consistently discriminated against: the elderly and Latinos.
Among economists, there are two leading theories of discrimination.
Interestingly, elderly Weakest Link contestants seem to suffer from one type, while Latinos suffer the other.
The first type is called taste-based discrimination, which means that one person discriminates simply because he prefers to not interact with a particular type of other person.
In the second type, known as information-based discrimination, one person believes that another type of person has poor skills, and acts accordingly.
On The Weakest Link, Latinos suffer information-based discrimination.
Other contestants seem to view the Latinos as poor players, even when they are not.
This perception translates into Latinos’ being eliminated in the early rounds even if they are doing well and not being eliminated in the later rounds, when other contestants want to keep the Latinos around to weaken the field.
Elderly players, meanwhile, are victims of taste-based discrimination: in the early rounds and late rounds, they are eliminated far out of proportion to their skills.
It seems as if the other contestants —this is a show on which the average age is thirty-four—simply don’t want the older players around.
It’s quite possible that a typical Weakest Link contestant isn’t even cognizant of his discrimination toward Latinos and the elderly (or, in the case of blacks and women, his lack of discrimination).
He is bound to be nervous, after all, and excited, playing a fast-moving game under the glare of television lights.
Which naturally suggests another question: how might that same person express his preferences—and reveal information about himself—in the privacy of his home?
It all happens on Internet dating sites.
Some of them, like Match.com, eHarmony.com, and Yahoo!
Dating websites are the most successful subscription-based business on the Internet.
Each site operates a bit differently, but the gist is this: You compose a personal ad about yourself that typically includes a photo, vital statistics, your income range, level of education, likes and dislikes, and so on.
On many sites, you also specify your dating aims: “long-term relationship,” “a casual lover,” or “just looking.” So there are two massive layers of data to be mined here: the information that people include in their ads and the level of response gleaned by any particular ad.
In the case of the ads, how forthright (and honest) are people when it comes to sharing their personal information?
And in the case of the responses, what kind of information in personal ads is considered the most (and least) desirable?
Two economists and a psychologist recently banded together to address these questions.
Fifty-six percent of the users were men, and the median age range for all users was twenty-one to thirty-five.
Although they represented an adequate racial mix to reach some conclusions about race, they were predominantly white.
They were also a lot richer, taller, skinnier, and better-looking than average.
More than 4 percent of the online daters claimed to earn more than $200,000 a year, whereas fewer than 1 percent of typical Internet users actually earn that much, suggesting that three of the four big earners were exaggerating.
Male and female users typically reported that they are about an inch taller than the national average.
As for weight, the men were in line with the national average, but the women typically said they weighed about twenty pounds less than the national average.
Most impressively, fully 72 percent of the women claimed “above average” looks, including 24 percent claiming “very good looks.” The online men too were gorgeous: 68 percent called themselves “above average,” including 19 percent with “very good looks.” This leaves only about 30 percent of the users with “average” looks, including a paltry 1 percent with “less than average” looks —which suggests that the typical online dater is either a fabulist, a narcissist, or simply resistant to the meaning of “average.” (Or perhaps they are all just pragmatists: as any real-estate agent knows, the typical house isn’t “charming” or “fantastic,” but unless you say it is, no one will even bother to take a look.)
Twenty-eight percent of the women on the site said they were blond, a number far beyond the national average, which indicates a lot of dyeing, or lying, or both.
Seven percent of the men conceded that they were married, with a significant minority of these men reporting that they were “happily married.” But the fact that they were honest doesn’t mean they were rash.
There are plenty of reasons someone might not post a photo—he’s technically challenged or is ashamed of being spotted by friends or is just plain unattractive—but as in the case of a brand-new car with a For Sale sign, prospective customers will assume he’s got something seriously wrong under the hood.
Fifty-six percent of the men who post ads don’t receive even one e-mail; 21 percent of the women don’t get a single response.
The traits that do draw a big response, meanwhile, will not be a big surprise to anyone with even a passing knowledge of the sexes.
Women are eager to date military men, policemen, and firemen (possibly the result of a 9/11 Effect, like the higher payments to Paul Feldman’s bagel business), along with lawyers and doctors; they generally avoid men with manufacturing jobs.
For men, being short is a big disadvantage (which is probably why so many lie about it), but weight doesn’t much matter.
In addition to all the information about income, education, and looks, men and women on the dating site listed their race.
This means that an Asian man who is good-looking, rich, and well educated will receive fewer than 25 percent as many e-mails from white women as a white man with the same qualifications would receive; similarly, black and Latino men receive about half as many e-mails from white women as they would if they were white.
The gulf between the information we publicly proclaim and the information we know to be true is often vast.
By now we are fully accustomed to the false public proclamations of politicians themselves.
In New York City’s 1989 mayoral race between David Dinkins (a black candidate) and Rudolph Giuliani (who is white), Dinkins won by only a few points.
Duke, though he never won the high political office he often sought, proved himself a master of information abuse.
As Grand Wizard of the Knights of the Ku Klux Klan, he was able to compile a mailing list of thousands of rank-and-file Klansmen and other supporters who would eventually become his political base.
(It isn’t known whether he used a real-estate agent.)
And most of the money he raised from his supporters was being used not to promote any white supremacist cause but rather to satisfy Duke’s gambling habit.
The two previous chapters were built around a pair of admittedly freakish questions: What do schoolteachers and sumo wrestlers have in common?
and How is the Ku Klux Klan like a group of realestate agents?
But if you can question something that people really care about and find an answer that may surprise them—that is, if you can overturn the conventional wisdom—then you may have some luck.
It was John Kenneth Galbraith, the hyperliterate economic sage, who coined the phrase “conventional wisdom.” He did not consider it a compliment.
“We associate truth with convenience,” he wrote, “with what most closely accords with self-interest and personal well-being or promises best to avoid awkward effort or unwelcome dislocation of life.
We also find highly acceptable what contributes most to self-esteem.” Economic and social behaviors, Galbraith continued, “are complex, and to comprehend their character is mentally tiring.
Therefore we adhere, as though to a raft, to those ideas which represent our understanding.” So the conventional wisdom in Galbraith’s view must be simple, convenient, comfortable, and comforting—though not necessarily true.
It would be silly to argue that the conventional wisdom is never true.
But noticing where the conventional wisdom may be false—noticing, perhaps, the contrails of sloppy or self-interested thinking—is a nice place to start asking questions.
Consider the recent history of homelessness in the United States.
That sure seemed high, but…well, the expert said it.
It may be sad but not surprising to learn that experts like Snyder can be self-interested to the point of deceit.
Journalists need experts as badly as experts need journalists.
Every day there are newspaper pages and television newscasts to be filled, and an expert who can deliver a jarring piece of wisdom is always welcome.
Working together, journalists and experts are the architects of much conventional wisdom.
Advertising too is a brilliant tool for creating conventional wisdom.
Listerine, for instance, was invented in the nineteenth century as a powerful surgical antiseptic.
It was later sold, in distilled form, as a floor cleaner and a cure for gonorrhea.
But it wasn’t a runaway success until the 1920s, when it was pitched as a solution for “chronic halitosis”—a then obscure medical term for bad breath.
Listerine’s new ads featured forlorn young women and men, eager for marriage but turned off by their mate’s rotten breath.
But Listerine changed that.
As the advertising scholar James B. Twitchell writes, “Listerine did not make mouthwash as much as it made halitosis.” In just seven years, the company’s revenues rose from $115,000 to more than $8 million.
However created, the conventional wisdom can be hard to budge.
The economist Paul Krugman, a New York Times columnist and devout critic of George W. Bush, bemoaned this fact as the President’s reelection campaign got under way in early 2004: “The approved story line about Mr. Bush is that he’s a bluff, honest, plainspoken guy, and anecdotes that fit that story get reported.
But if the conventional wisdom were instead that he’s a phony, a silver-spoon baby who pretends to be a cowboy, journalists would have plenty of material to work with.” In the months leading up to the U.S. invasion of Iraq in 2003, dueling experts floated diametrically opposite forecasts about Iraq’s weapons of mass destruction.
But more often, as with Mitch Snyder ’s homeless “statistics,” one side wins the war of conventional wisdom.
Women’s rights advocates, for instance, have hyped the incidence of sexual assault, claiming that one in three American women will in her lifetime be a victim of rape or attempted rape.
A little creative lying can draw attention, indignation, and—perhaps most important—the money and political capital to address the actual problem.
Of course an expert, whether a women’s health advocate or a political advisor or an advertising executive, tends to have different incentives than the rest of us.
And an expert’s incentives may shift 180 degrees, depending on the situation.
Consider the police.
A recent audit discovered that the police in Atlanta were radically underreporting crime since the early 1990s.
The city needed to shed its violent image, and fast.
So each year thousands of crime reports were either downgraded from violent to nonviolent or simply thrown away.
(Despite these continuing efforts—there were more than 22,000 missing police reports in 2002 alone—Atlanta regularly ranks among the most violent American cities.)
The sudden, violent appearance of crack cocaine had police departments across the country scrapping for resources.
They made it known that it wasn’t a fair fight: the drug dealers were armed with state-ofthe-art weapons and a bottomless supply of cash.
This emphasis on illicit cash proved to be a winning effort, for nothing infuriated the law-abiding populace more than the image of the millionaire crack dealer.
The media eagerly glommed on to this story, portraying crack dealing as one of the most profitable jobs in America.
And then you may have scratched your head and said, “Why is that?” The answer lies in finding the right data, and the secret to finding the right data usually means finding the right person—which is more easily said than done.
Drug dealers are rarely trained in economics, and economists rarely hang out with crack dealers.
So the answer to this question begins with finding someone who did live among the drug dealers and managed to walk away with the secrets of their trade.
Sudhir Venkatesh—his boyhood friends called him Sid, but he has since reverted to Sudhir—was born in India, raised in the suburbs of upstate New York and southern California, and graduated from the University of California at San Diego with a degree in mathematics.
He was interested in understanding how young people form their identities; to that end, he had just spent three months following the Grateful Dead around the country.
His assignment: to visit Chicago’s poorest black neighborhoods with a clipboard and a seventy-question, multiple-choice survey.
Venkatesh soon discovered that the names and addresses he had been given were badly outdated.
Suddenly, on the stairwell landing, he startled a group of teenagers shooting dice.
Things had been violent lately, with shootings nearly every day.
This gang, a branch of the Black Gangster Disciple Nation, was plainly on edge.
Thanks to his three months trailing the Grateful Dead, he still looked, as he would later put it, “like a genuine freak, with hair down to my ass.” The gang members started arguing over what should be done with Venkatesh.
“That’s because you can’t read,” said one of the teenagers, and everyone laughed at the older gangster.
“People don’t come out of here alive,” the jittery teenager with the gun told Venkatesh.
It struck Venkatesh that most people, including himself, had never given much thought to the daily life of ghetto criminals.
He was now eager to learn how the Black Disciples worked, from top to bottom.
He knew the importance of collecting data and finding new markets; he was always on the lookout for better management strategies.
After some wrangling, J. T. promised Venkatesh unfettered access to the gang’s operations as long as J. T. retained veto power over any information that, if published, might prove harmful.
Sometimes the gangsters were annoyed by his curiosity; more often they took advantage of his willingness to listen.
We ain’t got no choice, and if that means getting killed, well, shit, it’s what niggers do around here to feed their family.” Venkatesh would move from one family to the next, washing their dinner dishes and sleeping on the floor.
He bought toys for their children; he once watched a woman use her baby’s bib to sop up the blood of a teenaged drug dealer who was shot to death in front of Venkatesh.
At J. T.’s direction, the ledgers had been rigorously compiled: sales, wages, dues, even the death benefits paid out to the families of murdered members.
This street-level research made Venkatesh something of an anomaly.
At the very top of his list was crime.
It would be the first time that such priceless financial data had fallen into an economist’s hands, affording an analysis of a heretofore uncharted criminal enterprise.
An awful lot like most American businesses, actually, though perhaps none more so than McDonald’s.
In fact, if you were to hold a McDonald’s organizational chart and a Black Disciples org chart side by side, you could hardly tell the difference.
(At the same time that white suburbanites were studiously mimicking black rappers’ ghetto culture, black ghetto criminals were studiously mimicking the suburbanites’ dads’ corp-think.)
Three officers reported directly to J. T.: an enforcer (who ensured the gang members’ safety), a treasurer (who watched over the gang’s liquid assets), and a runner (who transported large quantities of drugs and money to and from the supplier).
Beneath the officers were the street-level salesmen known as foot soldiers.
In the first year, it took in an average of $18,500 each month; by the final year, it was collecting $68,400 a month.
Here’s a look at the monthly revenues in the third year: “Drug sales” represents only the money from dealing crack cocaine.
The gang did allow some rank-and-file members to sell heroin on its turf but accepted a fixed licensing fee in lieu of a share of profits.
The extortionary taxes were paid by other businesses that operated on the gang’s turf, including grocery stores, gypsy cabs, pimps, and people selling stolen goods or repairing cars on the street.
Now, here’s what it cost J. T., excluding wages, to bring in that $32,000 per month: Mercenary fighters were nonmembers hired on short-term contracts to help the gang fight turf wars.
The miscellaneous expenses include legal fees, parties, bribes, and gang-sponsored “community events.” (The Black Disciples worked hard to be seen as a pillar rather than a scourge of the housingproject community.)
The miscellaneous expenses also include the costs associated with a gang member ’s murder.
The gang not only paid for the funeral but often gave a stipend of up to three years’ wages to the victim’s family.
The rest of the money the gang took in went to its members, starting with J. T. Here is the single line item in the gang’s budget that made J. T. the happiest: Net monthly profit accruing to leader $8,500 At $8,500 per month, J. T.’s annual salary was about $100,000—tax-free, of course, and not including the various off-the-books money he pocketed.
So there were indeed some drug dealers who could afford to live large —or, in the case of the gang’s board of directors, extremely large.
Here are the monthly totals for the wages that J. T. paid his gang members: So J. T. paid his employees $9,500, a combined monthly salary that was only $1,000 more than his own official salary.
J. T.’s hourly wage was $66.
And the foot soldiers earned just $3.30 an hour, less than the minimum wage.
So the answer to the original question—if drug dealers make so much money, why are they still living with their mothers?—is that, except for the top cats, they don’t make much money.
In other words, a crack gang works pretty much like the standard capitalist enterprise: you have to be near the top of the pyramid to make a big wage.
Notwithstanding the leadership’s rhetoric about the family nature of the business, the gang’s wages are about as skewed as wages in corporate America.
A foot soldier had plenty in common with a McDonald’s burger flipper or a Wal-Mart shelf stocker.
In fact, most of J. T.’s foot soldiers also held minimum-wage jobs in the legitimate sector to supplement their skimpy illicit earnings.
For starters, they had to stand on a street corner all day and do business with crackheads.
Had they grown up under different circumstances, they might have thought about becoming economists or writers.
Fifty-six percent of the neighborhood’s children lived below the poverty line (compared to a national average of 18 percent).
Seventy-eight percent came from single-parent homes.
The neighborhood’s median income was about $15,000 a year, well less than half the U.S. average.
But criminals, like everyone else, respond to incentives.
On the south side of Chicago, people wanting to sell crack vastly outnumbered the available street corners.
This is one of four meaningful factors that determine a wage.
In the glamour professions—movies, sports, music, fashion—there is a different dynamic at play.
Even in second-tier glamour industries like publishing, advertising, and media, swarms of bright young people throw themselves at grunt jobs that pay poorly and demand unstinting devotion.
An editorial assistant earning $22,000 at a Manhattan publishing house, an unpaid high-school quarterback, and a teenage crack dealer earning $3.30 an hour are all playing the same game, a game that is best viewed as a tournament.
(Just as a Major League shortstop probably played Little League and just as a Grand Dragon of the Ku Klux Klan probably started out as a lowly spear-carrier, a drug lord typically began by selling drugs on a street corner.)
You must be willing to work long and hard at substandard wages.
In order to advance in the tournament, you must prove yourself not merely above average but spectacular.
(Some people hang on longer than others—witness the graying “actors” who wait tables in New York—but people generally get the message quite early.)
For a foot soldier—the gang’s man on the street—this development was particularly dangerous.
For one thing, he was forced to pay his foot soldiers higher wages because of the added risk.
If Burger King and McDonald’s launch a price war to gain market share, they partly make up in volume what they lose in price.
That’s because they had different incentives.
For J. T., violence was a distraction from the business at hand; he would have preferred that his members never fired a single gunshot.
He also had his business education, of course.
He constantly worked to extend this advantage.
That was why he ordered the corporate-style bookkeeping that eventually found its way into Sudhir Venkatesh’s hands.
But this tournament had a catch that publishing and pro sports and even Hollywood don’t have.
Selling drugs, after all, is illegal.
Now for another unlikely question: what did crack cocaine have in common with nylon stockings?
In 1939, when DuPont introduced nylons, countless American women felt as if a miracle had been performed in their honor.
By 1941, some sixty-four million pairs of nylon stockings had been sold—more stockings than there were adult women in the United States.
DuPont had pulled off the feat that every marketer dreams of: it brought class to the masses.
In this regard, the invention of nylon stockings was markedly similar to the invention of crack cocaine.
More affectionate nicknames would soon follow: Rock, Kryptonite, Kibbles ’n Bits, Scrabble, and Love.
Blandon did so much business with the budding crack dealers of South Central Los Angeles that he came to be known as the Johnny Appleseed of Crack.
By putting massive amounts of cocaine into the hands of street gangs, Blandon and others like him gave rise to a devastating crack boom.
And gangs like the Black Gangster Disciple Nation were given new reason to exist.
In the 1920s, Chicago alone had more than 1,300 street gangs, catering to every ethnic, political, and criminal leaning imaginable.
Some fancied themselves commercial enterprises, and a few—the Mafia, most notably—actually did make money (at least for the higher-ups).
Black street gangs in particular flourished in Chicago, with membership in the tens of thousands by the 1970s.
They constituted the sort of criminals, petty and otherwise, who sucked the life out of urban areas.
Part of the problem was that these criminals never seemed to get locked up.
The 1960s and 1970s were, in retrospect, a great time to be a street criminal in most American cities.
The likelihood of punishment was so low—this was the heyday of a liberal justice system and the criminals’ rights movement—that it simply didn’t cost very much to commit a crime.
By happy coincidence, some of their fellow inmates were Mexican gang members with close ties to Colombian drug dealers.
In the past, the black gangsters had bought their drugs from a middleman, the Mafia—which, as it happened, was then being pummeled by the federal government’s new anti-racketeering laws.
This new product was ideal for a low-income, street-level customer.
And who better to sell it than the thousands of junior members of all those street gangs like the Black Gangster Disciple Nation?
Suddenly the urban street gang evolved from a club for wayward teenagers into a true commercial enterprise.
The gang also presented an opportunity for longtime employment.
Before crack, it was just about impossible to earn a living in a street gang.
This was happening just as the old-fashioned sort of lifetime jobs—factory jobs especially—were disappearing.
In the past, a semi-skilled black man in Chicago could earn a decent wage working in a factory.
Who cared if it was so dangerous—standing out there on a corner, selling it as fast and anonymously as McDonald’s sells hamburgers, not knowing any of your customers, wondering who might be coming to arrest or rob or kill you?
For black Americans, the four decades between World War II and the crack boom had been marked by steady and often dramatic improvement.
Then came crack cocaine.
Crack was so dramatically destructive that if its effect is averaged for all black Americans, not just crack users and their families, you will see that the group’s postwar progress was not only stopped cold but was often knocked as much as ten years backward.
Black Americans were hurt more by crack cocaine than by any other single cause since Jim Crow.
And then there was the crime.
Within a five-year period, the homicide rate among young urban blacks quadrupled.
It coincided with an even broader American crime wave that had been building for two decades.
Although the rise of this crime wave long predated crack, the trend was so exacerbated by crack that criminologists got downright apocalyptic in their predictions.
James Alan Fox, perhaps the most widely quoted crime expert in the popular press, warned of a coming “bloodbath” of youth violence.
But Fox and the other purveyors of conventional wisdom turned out to be wrong.
The crime rate in fact began to fall—so unexpectedly and dramatically and thoroughly that now, from the distance of several years, it is almost hard to recall the crushing grip of that crime wave.
Oscar Danilo Blandon, the socalled Johnny Appleseed of Crack, may have been the instigator of one ripple effect, in which by his actions a single person inadvertently causes an ocean of despair.
In 1966, one year after Nicolae Ceauşescu became the Communist dictator of Romania, he made abortion illegal.
“The fetus is the property of the entire society,” he proclaimed.
“The worms never get satisfied, regardless of how much food you give them,” she said when Romanians complained about the food shortages brought on by her husband’s mismanagement.
Ceauşescu’s ban on abortion was designed to achieve one of his major aims: to rapidly strengthen Romania by boosting its population.
Until 1966, Romania had had one of the most liberal abortion policies in the world.
Abortion was in fact the main form of birth control, with four abortions for every live birth.
Now, virtually overnight, abortion was forbidden.
At the same time, all contraception and sex education were banned.
Government agents sardonically known as the Menstrual Police regularly rounded up women in their workplaces to administer pregnancy tests.
If a woman repeatedly failed to conceive, she was forced to pay a steep “celibacy tax.” Ceauşescu’s incentives produced the desired effect.
Within one year of the abortion ban, the Romanian birth rate had doubled.
Compared to Romanian children born just a year earlier, the cohort of children born after the abortion ban would do worse in every measurable way: they would test lower in school, they would have less success in the labor market, and they would also prove much more likely to become criminals.
The abortion ban stayed in effect until Ceauşescu finally lost his grip on Romania.
On December 16, 1989, thousands of people took to the streets of Timisoara to protest his corrosive regime.
Many of the protestors were teenagers and college students.
The police killed dozens of them.
“Most were aged thirteen to twenty.” A few days after the massacre in Timisoara, Ceauşescu gave a speech in Bucharest before one hundred thousand people.
Of all the Communist leaders deposed in the years bracketing the collapse of the Soviet Union, only Nicolae Ceauşescu met a violent death.
It should not be overlooked that his demise was precipitated in large measure by the youth of Romania—a great number of whom, were it not for his abortion ban, would never have been born at all.
The story of abortion in Romania might seem an odd way to begin telling the story of American crime in the 1990s.
In one important way, the Romanian abortion story is a reverse image of the American crime story.
The point of overlap was on that Christmas Day of 1989, when Nicolae Ceauşescu learned the hard way—with a bullet to the head—that his abortion ban had much deeper implications than he knew.
On that day, crime was just about at its peak in the United States.
In the previous fifteen years, violent crime had risen 80 percent.
It was crime that led the nightly news and the national conversation.
When the crime rate began falling in the early 1990s, it did so with such speed and suddenness that it surprised everyone.
It took some experts many years to even recognize that crime was falling, so confident had they been of its continuing rise.
Long after crime had peaked, in fact, some of them continued to predict ever darker scenarios.
But the evidence was irrefutable: the long and brutal spike in crime was moving in the opposite direction, and it wouldn’t stop until the crime rate had fallen back to the levels of forty years earlier.
Now the experts hustled to explain their faulty forecasting.
“I never said there would be blood flowing in the streets,” he said, “but I used strong terms like ‘bloodbath’ to get people’s attention.
I don’t apologize for using alarmist terms.” (If Fox seems to be offering a distinction without a difference—“bloodbath” versus “blood flowing in the streets”—we should remember that even in retreat mode, experts can be self-serving.)
After the relief had settled in, after people remembered how to go about their lives without the pressing fear of crime, there arose a natural question: just where did all those criminals go?
After all, if none of the criminologists, police officials, economists, politicians, or others who traffic in such matters had foreseen the crime decline, how could they suddenly identify its causes?
But this diverse army of experts now marched out a phalanx of hypotheses to explain the drop in crime.
Their conclusions often hinged on which expert had most recently spoken to which reporter.
Here, ranked by frequency of mention, are the crime-drop explanations cited in articles published from 1991 to 2001 in the ten largest-circulation papers in the LexisNexis database: If you are the sort of person who likes guessing games, you may wish to spend the next few moments pondering which of the preceding explanations seem to have merit and which don’t.
Hint: of the seven major explanations on the list, only three can be shown to have contributed to the drop in crime.
The others are, for the most part, figments of someone’s imagination, self-interest, or wishful thinking.
Further hint: one of the greatest measurable causes of the crime drop does not appear on the list at all, for it didn’t receive a single newspaper mention.
The decline in crime that began in the early 1990s was accompanied by a blistering national economy and a significant drop in unemployment.
It might seem to follow that the economy was a hammer that helped beat down crime.
It is true that a stronger job market may make certain crimes relatively less attractive.
But that is only the case for crimes with a direct financial motivation —burglary, robbery, and auto theft—as opposed to violent crimes like homicide, assault, and rape.
Moreover, studies have shown that an unemployment decline of 1 percentage point accounts for a 1 percent drop in nonviolent crime.
During the 1990s, the unemployment rate fell by 2 percentage points; nonviolent crime, meanwhile, fell by roughly 40 percent.
But an even bigger flaw in the strong-economy theory concerns violent crime.
Homicide fell at a greater rate during the 1990s than any other sort of crime, and a number of reliable studies have shown virtually no link between the economy and violent crime.
This weak link is made even weaker by glancing back to a recent decade, the 1960s, when the economy went on a wild growth spurt—as did violent crime.
So while a strong 1990s economy might have seemed, on the surface, a likely explanation for the drop in crime, it almost certainly didn’t affect criminal behavior in any significant way.
Let’s now consider another crime-drop explanation: increased reliance on prisons.
It might help to start by flipping the crime question around.
Instead of wondering what made crime fall, think about this: why had it risen so dramatically in the first place?
During the first half of the twentieth century, the incidence of violent crime in the United States was, for the most part, fairly steady.
Conviction rates declined during the 1960s, and criminals who were convicted served shorter sentences.
This trend was driven in part by an expansion in the rights of people accused of crimes—a long overdue expansion, some would argue.
At the same time, politicians were growing increasingly softer on crime—“for fear of sounding racist,” as the economist Gary Becker has written, “since African-Americans and Hispanics commit a disproportionate share of felonies.” So if you were the kind of person who might want to commit a crime, the incentives were lining up in your favor: a slimmer likelihood of being convicted and, if convicted, a shorter prison term.
Because criminals respond to incentives as readily as anyone, the result was a surge in crime.
It took some time, and a great deal of political turmoil, but these incentives were eventually curtailed.
Criminals who would have previously been set free—for drug-related offenses and parole revocation in particular—were instead locked up.
Between 1980 and 2000, there was a fifteenfold increase in the number of people sent to prison on drug charges.
Many other sentences, especially for violent crime, were lengthened.
The evidence linking increased punishment with lower crime rates is very strong.
Harsh prison terms have been shown to act as both deterrent (for the would-be criminal on the street) and prophylactic (for the would-be criminal who is already locked up).
Logical as this may sound, some criminologists have fought the logic.
A 1977 academic study called “On Behalf of a Moratorium on Prison Construction” noted that crime rates tend to be high when imprisonment rates are high, and concluded that crime would fall if imprisonment rates could only be lowered.
(Fortunately, jailers did not suddenly turn loose their wards and sit back waiting for crime to fall.
As the political scientist John J. DiIulio Jr. later commented, “Apparently, it takes a Ph.D. in criminology to doubt that keeping dangerous criminals incarcerated cuts crime.”) The “Moratorium” argument rests on a fundamental confusion of correlation and causality.
The mayor of a city sees that his citizens celebrate wildly when their team wins the World Series.
So the following year, the mayor decrees that his citizens start celebrating the World Series before the first pitch is thrown—an act that, in his confused mind, will ensure a victory.
Nor does prison even begin to address the root causes of crime, which are diverse and complex.
But if the goal here is to explain the drop in crime in the 1990s, imprisonment is certainly one of the key answers.
It accounts for roughly one-third of the drop in crime.
Another crime-drop explanation is often cited in tandem with imprisonment: the increased use of capital punishment.
The number of executions in the United States quadrupled between the 1980s and the 1990s, leading many people to conclude—in the context of a debate that has been going on for decades—that capital punishment helped drive down crime.
First, given the rarity with which executions are carried out in this country and the long delays in doing so, no reasonable criminal should be deterred by the threat of execution.
Even though capital punishment quadrupled within a decade, there were still only 478 executions in the entire United States during the 1990s.
New York State, for instance, has not as of this writing executed a single criminal since reinstituting its death penalty in 1995.
Even among prisoners on death row, the annual execution rate is only 2 percent—compared with the 7 percent annual chance of dying faced by a member of the Black Gangster Disciple Nation crack gang.
If life on death row is safer than life on the streets, it’s hard to believe that the fear of execution is a driving force in a criminal’s calculus.
Like the $3 fine for late-arriving parents at the Israeli day-care centers, the negative incentive of capital punishment simply isn’t serious enough for a criminal to change his behavior.
The second flaw in the capital punishment argument is even more obvious.
How much crime does it actually deter?
The economist Isaac Ehrlich, in an oft-cited 1975 paper, put forth an estimate that is generally considered optimistic: executing 1 criminal translates into 7 fewer homicides that the criminal might have committed.
According to Ehrlich’s calculation, those 52 additional executions would have accounted for 364 fewer homicides in 2001—not a small drop, to be sure, but less than 4 percent of the actual decrease in homicides that year.
So even in a death penalty advocate’s best-case scenario, capital punishment could explain only one twenty-fifth of the drop in homicides in the 1990s.
And because the death penalty is rarely given for crimes other than homicide, its deterrent effect cannot account for a speck of decline in other violent crimes.
It is extremely unlikely, therefore, that the death penalty, as currently practiced in the United States, exerts any real influence on crime rates.
“I feel morally and intellectually obligated simply to concede that the death penalty experiment has failed,” said U.S. Supreme Court Justice Harry A. Blackmun in 1994, nearly twenty years after he had voted for its reinstatement.
“I no longer shall tinker with the machinery of death.” So it wasn’t capital punishment that drove crime down, nor was it the booming economy.
All those criminals didn’t march into jail by themselves, of course.
Someone had to investigate the crime, catch the bad guy, and put together the case that would get him convicted.
Which naturally leads to a related pair of crime-drop explanations: Innovative policing strategies Increased number of police Let’s address the second one first.
The number of police officers per capita in the United States rose about 14 percent during the 1990s.
Does merely increasing the number of police, however, reduce crime?
That’s because when crime is rising, people clamor for protection, and invariably more money is found for cops.
So if you just look at raw correlations between police and crime, you will find that when there are more police, there tends to be more crime.
That doesn’t mean, of course, that the police are causing the crime, just as it doesn’t mean, as some criminologists have argued, that crime will fall if criminals are released from prison.
To show causality, we need a scenario in which more police are hired for reasons completely unrelated to rising crime.
If, for instance, police were randomly sprinkled in some cities and not in others, we could look to see whether crime declines in the cities where the police happen to land.
As it turns out, that exact scenario is often created by vote-hungry politicians.
In the months leading up to Election Day, incumbent mayors routinely try to lock up the law-and-order vote by hiring more police—even when the crime rate is standing still.
So by comparing the crime rate in one set of cities that have recently had an election (and which therefore hired extra police) with another set of cities that had no election (and therefore no extra police), it’s possible to tease out the effect of the extra police on crime.
The answer: yes indeed, additional police substantially lower the crime rate.
Again, it may help to look backward and see why crime had risen so much in the first place.
From 1960 to 1985, the number of police officers fell more than 50 percent relative to the number of crimes.
In some cases, hiring additional police was considered a violation of the era’s liberal aesthetic; in others, it was simply deemed too expensive.
This 50 percent decline in police translated into a roughly equal decline in the probability that a given criminal would be caught.
Coupled with the above-cited leniency in the other half of the criminal justice system, the courtrooms, this decrease in policing created a strong positive incentive for criminals.
Not only did all those police act as a deterrent, but they also provided the manpower to imprison criminals who might have otherwise gone uncaught.
The hiring of additional police accounted for roughly 10 percent of the 1990s crime drop.
But it wasn’t only the number of police that changed in the 1990s; consider the most commonly cited crime-drop explanation of all: innovative policing strategies.
There was perhaps no more attractive theory than the belief that smart policing stops crime.
This theory rapidly became an article of faith because it appealed to the factors that, according to John Kenneth Galbraith, most contribute to the formation of conventional wisdom: the ease with which an idea may be understood and the degree to which it affects our personal well-being.
The story played out most dramatically in New York City, where newly elected mayor Rudolph Giuliani and his handpicked police commissioner, William Bratton, vowed to fix the city’s desperate crime situation.
He ushered the NYPD into what one senior police official later called “our Athenian period,” in which new ideas were given weight over calcified practices.
Instead of relying solely on old-fashioned cop know-how, he introduced technological solutions like CompStat, a computerized method of addressing crime hot spots.
The most compelling new idea that Bratton brought to life stemmed from the broken window theory, which was conceived by the criminologists James Q. Wilson and George Kelling.
The broken window theory argues that minor nuisances, if left unchecked, turn into major nuisances: that is, if someone breaks a window and sees it isn’t fixed immediately, he gets the signal that it’s all right to break the rest of the windows and maybe set the building afire too.
So with murder raging all around, Bill Bratton’s cops began to police the sort of deeds that used to go unpoliced: jumping a subway turnstile, panhandling too aggressively, urinating in the streets, swabbing a filthy squeegee across a car ’s windshield unless the driver made an appropriate “donation.” Most New Yorkers loved this crackdown on its own merit.
But they particularly loved the idea, as stoutly preached by Bratton and Giuliani, that choking off these small crimes was like choking off the criminal element’s oxygen supply.
Today’s turnstile jumper might easily be wanted for yesterday’s murder.
That junkie peeing in an alley might have been on his way to a robbery.
As violent crime began to fall dramatically, New Yorkers were more than happy to heap laurels on their operatic, Brooklyn-bred mayor and his hatchet-faced police chief with the big Boston accent.
Soon after the city’s crime turnaround landed Bratton—and not Giuliani—on the cover of Time, Bratton was pushed to resign.
He had been police commissioner for just twenty-seven months.
New York City was a clear innovator in police strategies during the 1990s crime drop, and it also enjoyed the greatest decline in crime of any large American city.
Homicide rates fell from 30.7 per 100,000 people in 1990 to 8.4 per 100,000 people in 2000, a change of 73.6 percent.
First, the drop in crime in New York began in 1990.
By the end of 1993, the rate of property crime and violent crime, including homicides, had already fallen nearly 20 percent.
Second, the new police strategies were accompanied by a much more significant change within the police force: a hiring binge.
Between 1991 and 2001, the NYPD grew by 45 percent, more than three times the national average.
As argued above, an increase in the number of police, regardless of new strategies, has been proven to reduce crime.
By a conservative calculation, this huge expansion of New York’s police force would be expected to reduce crime in New York by 18 percent relative to the national average.
If you subtract that 18 percent from New York’s homicide reduction, thereby discounting the effect of the police-hiring surge, New York no longer leads the nation with its 73.6 percent drop; it goes straight to the middle of the pack.
Many of those new police were in fact hired by David Dinkins, the mayor whom Giuliani defeated.
So those who wish to credit Giuliani with the crime drop may still do so, for it was his own law-and-order reputation that made Dinkins hire all those police.
In the end, of course, the police increase helped everyone—but it helped Giuliani a lot more than Dinkins.
Most damaging to the claim that New York’s police innovations radically lowered crime is one simple and often overlooked fact: crime went down everywhere during the 1990s, not only in New York.
But even in Los Angeles, a city notorious for bad policing, crime fell at about the same rate as it did in New York once the growth in New York’s police force is accounted for.
Bill Bratton certainly deserves credit for invigorating New York’s police force.
But there is frighteningly little evidence that his strategy was the crime panacea that he and the media deemed it.
The next step will be to continue measuring the impact of police innovations—in Los Angeles, for instance, where Bratton himself became police chief in late 2002.
While he duly instituted some of the innovations that were his hallmark in New York, Bratton announced that his highest priority was a more basic one: finding the money to hire thousands of new police officers.
Now to explore another pair of common crime-drop explanations: Tougher gun laws Changes in crack and other drug markets First, the guns.
Any mugger with even a little initiative is bound to be armed, for in a country like the United States, with a thriving black market in guns, anyone can get hold of one.
Nearly two-thirds of U.S. homicides involve a gun, a far greater fraction than in other industrialized countries.
Our homicide rate is also much higher than in those countries.
It would therefore seem likely that our homicide rate is so high in part because guns are so easily available.
In Switzerland, every adult male is issued an assault rifle for militia duty and is allowed to keep the gun at home.
On a per capita basis, Switzerland has more firearms than just about any other country, and yet it is one of the safest places in the world.
In other words, guns do not cause crime.
That said, the established U.S. methods of keeping guns away from the people who do cause crime are, at best, feeble.
So bearing all this in mind, let’s consider a variety of recent gun initiatives to see the impact they may have had on crime in the 1990s.
The most famous gun-control law is the Brady Act, passed in 1993, which requires a criminal check and a waiting period before a person can purchase a handgun.
This solution may have seemed appealing to politicians, but to an economist it doesn’t make much sense.
Because regulation of a legal market is bound to fail when a healthy black market exists for the same product.
With guns so cheap and so easy to get, the standard criminal has no incentive to fill out a firearms application at his local gun shop and then wait a week.
The Brady Act, accordingly, has proven to be practically impotent in lowering crime.
(A study of imprisoned felons showed that even before the Brady Act, only about one-fifth of the criminals had bought their guns through a licensed dealer.)
Washington, D.C., and Chicago both instituted handgun bans well before crime began to fall across the country in the 1990s, and yet those two cities were laggards, not leaders, in the national reduction in crime.
One deterrent that has proven moderately effective is a stiff increase in prison time for anyone caught in possession of an illegal gun.
Not that this is likely, but if the death penalty were assessed to anyone carrying an illegal gun, and if the penalty were actually enforced, gun crimes would surely plunge.
Another staple of 1990s crime fighting—and of the evening news—was the gun buyback.
You remember the image: a menacing, glistening heap of firearms surrounded by the mayor, the police chief, the neighborhood activists.
Given the number of handguns in the United States and the number of homicides each year, the likelihood that a particular gun was used to kill someone that year is 1 in 10,000.
The typical gun buyback program yields fewer than 1,000 guns—which translates into an expectation of less than one-tenth of one homicide per buyback.
Not enough, that is, to make even a sliver of impact on the fall of crime.
Then there is an opposite argument—that we need more guns on the street, but in the hands of the right people (like the high-school girl above, instead of her mugger).
His calling card is the book More Guns, Less Crime, in which he argues that violent crime has decreased in areas where law-abiding citizens are allowed to carry concealed weapons.
If a criminal thinks his potential victim may be armed, he may be deterred from committing the crime.
Lott finally had to tell us that it was best for us to try and take classes from other professors more to be exposed to other ways of teaching graduate material.” Then there was the troubling allegation that Lott actually invented some of the survey data that support his more-guns/less-crime theory.
When other scholars have tried to replicate his results, they found that right-to-carry laws simply don’t bring down crime.
Consider the next crime-drop explanation: the bursting of the crack bubble.
But that only made the street-level dealers all the more desperate to advance.
The typical crack murder involved one crack dealer shooting another (or two of them, or three) and not, contrary to conventional wisdom, some bug-eyed crackhead shooting a shopkeeper over a few dollars.
The result was a huge increase in violent crime.
One study found that more than 25 percent of the homicides in New York City in 1988 were crack-related.
From 1991 to 2001, the homicide rate among young black men—who were disproportionately represented among crack dealers—fell 48 percent, compared to 30 percent for older black men and older white men.
(Another minor contributor to the falling homicide rate is the fact that some crack dealers took to shooting their enemies in the buttocks rather than murdering them; this method of violent insult was considered more degrading—and was obviously less severely punished—than murder.)
All told, the crash of the crack market accounted for roughly 15 percent of the crime drop of the 1990s—a substantial factor, to be sure, though it should be noted that crack was responsible for far more than 15 percent of the crime increase of the 1980s.
In other words, the net effect of crack is still being felt in the form of violent crime, to say nothing of the miseries the drug itself continues to cause.
The final pair of crime-drop explanations concern two demographic trends.
The first one received many media citations: aging of the population.
Until crime fell so drastically, no one talked about this theory at all.
In fact, the “bloodbath” school of criminology was touting exactly the opposite theory—that an increase in the teenage share of the population would produce a crop of superpredators who would lay the nation low.
“The population will start getting younger again….Get ready.” But overall, the teenage share of the population wasn’t getting much bigger.
While this may have been scary news in terms of Medicare and Social Security, the average American had little to fear from the growing horde of oldsters.
It shouldn’t be surprising to learn that elderly people are not very criminally intent; the average sixtyfive-year-old is about one-fiftieth as likely to be arrested as the average teenager.
That is what makes this aging-of-the-population theory of crime reduction so appealingly tidy: since people mellow out as they get older, more older people must lead to less crime.
But a thorough look at the data reveals that the graying of America did nothing to bring down crime in the 1990s.
Demographic change is too slow and subtle a process—you don’t graduate from teenage hoodlum to senior citizen in just a few years—to even begin to explain the suddenness of the crime decline.
There was another demographic change, however, unforeseen and long-gestating, that did drastically reduce crime in the 1990s.
Suddenly and without warning, Nicolae Ceauşescu declared abortion illegal.
The children born in the wake of the abortion ban were much more likely to become criminals than children born earlier.
In most of these cases, abortion was not forbidden outright, but a woman had to receive permission from a judge in order to obtain one.
Researchers found that in the instances where the woman was denied an abortion, she often resented her baby and failed to provide it with a good home.
Even when controlling for the income, age, education, and health of the mother, the researchers found that these children too were more likely to become criminals.
The United States, meanwhile, has had a different abortion history than Europe.
In the early days of the nation, it was permissible to have an abortion prior to “quickening”—that is, when the first movements of the fetus could be felt, usually around the sixteenth to eighteenth week of pregnancy.
In 1828, New York became the first state to restrict abortion; by 1900 it had been made illegal throughout the country.
Fewer poor women, therefore, had abortions.
They also had less access to birth control.
In the late 1960s, several states began to allow abortion under extreme circumstances: rape, incest, or danger to the mother.
By 1970 five states had made abortion entirely legal and broadly available: New York, California, Washington, Alaska, and Hawaii.
On January 22, 1973, legalized abortion was suddenly extended to the entire country with the U.S. Supreme Court’s ruling in Roe v. Wade.
There is also the distress, for all concerned, associated with the unwanted child, and there is the problem of bringing a child into a family already unable, psychologically and otherwise, to care for it.
She may be unmarried or in a bad marriage.
She may think her life is too unstable or unhappy, or she may think that her drinking or drug use will damage the baby’s health.
She may believe that she is too young or hasn’t yet received enough education.
In the first year after Roe v. Wade, some 750,000 women had abortions in the United States (representing one abortion for every 4 live births).
By 1980 the number of abortions reached 1.6 million (one for every 2.25 live births), where it leveled off.
In a country of 225 million people, 1.6 million abortions per year—one for every 140 Americans—may not have seemed so dramatic.
In the first year after Nicolae Ceauşescu’s death, when abortion was reinstated in Romania, there was one abortion for every twenty-two Romanians.
Before Roe v. Wade, it was predominantly the daughters of middle-or upper-class families who could arrange and afford a safe illegal abortion.
Now, instead of an illegal procedure that might cost $500, any woman could easily obtain an abortion, often for less than $100.
What sort of woman was most likely to take advantage of Roe v. Wade?
One study has shown that the typical child who went unborn in the earliest years of legalized abortion would have been 50 percent more likely than average to live in poverty; he would have also been 60 percent more likely to grow up with just one parent.
These two factors—childhood poverty and a single-parent household—are among the strongest predictors that a child will have a criminal future.
Growing up in a single-parent home roughly doubles a child’s propensity to commit crime.
So does having a teenage mother.
Another study has shown that low maternal education is the single most powerful factor leading to criminality.
In other words, the very factors that drove millions of American women to have an abortion also seemed to predict that their children, had they been born, would have led unhappy and possibly criminal lives.
To be sure, the legalization of abortion in the United States had myriad consequences.
So did shotgun marriages, as well as the number of babies put up for adoption (which has led to the boom in the adoption of foreign babies).
Conceptions rose by nearly 30 percent, but births actually fell by 6 percent, indicating that many women were using abortion as a method of birth control, a crude and drastic sort of insurance policy.
Perhaps the most dramatic effect of legalized abortion, however, and one that would take years to reveal itself, was its impact on crime.
In the early 1990s, just as the first cohort of children born after Roe v. Wade was hitting its late teen years—the years during which young men enter their criminal prime—the rate of crime began to fall.
What this cohort was missing, of course, were the children who stood the greatest chance of becoming criminals.
And the crime rate continued to fall as an entire generation came of age minus the children whose mothers had not wanted to bring a child into the world.
Legalized abortion led to less unwantedness; unwantedness leads to high crime; legalized abortion, therefore, led to less crime.
This theory is bound to provoke a variety of reactions, ranging from disbelief to revulsion, and a variety of objections, ranging from the quotidian to the moral.
Perhaps abortion and crime are merely correlated and not causal.
It may be more comforting to believe what the newspapers say, that the drop in crime was due to brilliant policing and clever gun control and a surging economy.
But we too embrace faulty causes, usually at the urging of an expert proclaiming a truth in which he has a vested interest.
How, then, can we tell if the abortion-crime link is a case of causality rather than simply correlation?
One way to test the effect of abortion on crime would be to measure crime data in the five states where abortion was made legal before the Supreme Court extended abortion rights to the rest of the country.
In New York, California, Washington, Alaska, and Hawaii, a woman had been able to obtain a legal abortion for at least two years before Roe v. Wade.
And indeed, those early-legalizing states saw crime begin to fall earlier than the other forty-five states and the District of Columbia.
Between 1988 and 1994, violent crime in the early-legalizing states fell 13 percent compared to the other states; between 1994 and 1997, their murder rates fell 23 percent more than those of the other states.
What else might we look for in the data to establish an abortion-crime link?
One factor to look for would be a correlation between each state’s abortion rate and its crime rate.
Sure enough, the states with the highest abortion rates in the 1970s experienced the greatest crime drops in the 1990s, while states with low abortion rates experienced smaller crime drops.
(This correlation exists even when controlling for a variety of factors that influence crime: a state’s level of incarceration, number of police, and its economic situation.)
Since 1985, states with high abortion rates have experienced a roughly 30 percent drop in crime relative to low-abortion states.
(New York City had high abortion rates and lay within an early-legalizing state, a pair of facts that further dampen the claim that innovative policing caused the crime drop.)
Moreover, there was no link between a given state’s abortion rate and its crime rate before the late 1980s—when the first cohort affected by legalized abortion was reaching its criminal prime—which is yet another indication that Roe v. Wade was indeed the event that tipped the crime scale.
There are even more correlations, positive and negative, that shore up the abortion-crime link.
In states with high abortion rates, the entire decline in crime was among the post-Roe cohort as opposed to older criminals.
Also, studies of Australia and Canada have since established a similar link between legalized abortion and crime.
And the post-Roe cohort was not only missing thousands of young male criminals but also thousands of single, teenage mothers—for many of the aborted baby girls would have been the children most likely to replicate their own mothers’ tendencies.
To discover that abortion was one of the greatest crime-lowering factors in American history is, needless to say, jarring.
The crime drop was, in the language of economists, an “unintended benefit” of legalized abortion.
But one need not oppose abortion on moral or religious grounds to feel shaken by the notion of a private sadness being converted into a public good.
Indeed, there are plenty of people who consider abortion itself to be a violent crime.
One legal scholar called legalized abortion worse than either slavery (since it routinely involves death) or the Holocaust (since the number of post-Roe abortions in the United States, roughly thirty-seven million as of 2004, outnumber the six million Jews killed in Europe).
Whether or not one feels so strongly about abortion, it remains a singularly charged issue.
Anthony V. Bouza, a former top police official in both the Bronx and Minneapolis, discovered this when he ran for Minnesota governor in 1994.
A few years earlier, Bouza had written a book in which he called abortion “arguably the only effective crime-prevention device adopted in this nation since the late 1960s.” When Bouza’s opinion was publicized just before the election, he fell sharply in the polls.
However a person feels about abortion, a question is likely to come to mind: what are we to make of the trade-off of more abortion for less crime?
As it happens, economists have a curious habit of affixing numbers to complicated transactions.
One economic study found that in order to protect roughly five thousand owls, the opportunity costs—that is, the income surrendered by the logging industry and others—would be $46 billion, or just over $9 million per owl.
After the Exxon Valdez oil spill in 1989, another study estimated the amount that the typical American household would be willing to pay to avoid another such disaster: $31.
Now, for the sake of argument, let’s ask an outrageous question: what is the relative value of a fetus and a newborn?
This is nothing but a thought exercise—obviously there is no right answer—but it may help clarify the impact of abortion on crime.
The second person, believing that a woman’s right to an abortion trumps any other factor, would likely argue that no number of fetuses can equal even one newborn.
There are roughly 1.5 million abortions in the United States every year.
For a person who believes that 1 newborn is worth 100 fetuses, those 1.5 million abortions would translate—dividing 1.5 million by 100—into the equivalent of a loss of 15,000 human lives.
Fifteen thousand lives: that happens to be about the same number of people who die in homicides in the United States every year.
And it is far more than the number of homicides eliminated each year due to legalized abortion.
So even for someone who considers a fetus to be worth only one one-hundredth of a human being, the trade-off between higher abortion and lower crime is, by an economist’s reckoning, terribly inefficient.
What the link between abortion and crime does say is this: when the government gives a woman the opportunity to make her own decision about abortion, she generally does a good job of figuring out if she is in a position to raise the baby well.
If she decides she can’t, she often chooses the abortion.
But once a woman decides she will have her baby, a pressing question arises: what are parents supposed to do once a child is born?
Over the recent decades, a vast and diverse flock of parenting experts has arisen.
Anyone who tries even casually to follow their advice may be stymied, for the conventional wisdom on parenting seems to shift by the hour.
Sometimes it is a case of one expert differing from another.
At other times the most vocal experts suddenly agree en masse that the old wisdom was wrong and that the new wisdom is, for a little while at least, irrefutably right.
A baby should always be put to sleep on her back—until it is decreed that she should only be put to sleep on her stomach.
In her book Raising America: Experts, Parents, and a Century of Advice About Children, Ann Hulbert documented how parenting experts contradict one another and even themselves.
Gary Ezzo, who in the Babywise book series endorses an “infant-management strategy” for moms and dads trying to “achieve excellence in parenting,” stresses how important it is to train a baby, early on, to sleep alone through the night.
Otherwise, Ezzo warns, sleep deprivation might “negatively impact an infant’s developing central nervous system” and lead to learning disabilities.
Advocates of “co-sleeping,” meanwhile, warn that sleeping alone is harmful to a baby’s psyche and that he should be brought into the “family bed.” What about stimulation?
As Holt explained, a baby should be left to cry for fifteen to thirty minutes a day: “It is the baby’s exercise.” The typical parenting expert, like experts in other fields, is prone to sound exceedingly sure of himself.
An expert doesn’t so much argue the various sides of an issue as plant his flag firmly on one side.
That’s because an expert whose argument reeks of restraint or nuance often doesn’t get much attention.
An expert must be bold if he hopes to alchemize his homespun theory into conventional wisdom.
His best chance of doing so is to engage the public’s emotions, for emotion is the enemy of rational argument.
The superpredator, Iraqi weapons of mass destruction, mad-cow disease, crib death: how can we fail to heed the expert’s advice on these horrors when, like that mean uncle telling too-scary stories to tooyoung children, he has reduced us to quivers?
No one is more susceptible to an expert’s fearmongering than a parent.
This leads a lot of parents to spend a lot of their parenting energy simply being scared.
And the white noise generated by the experts—to say nothing of the pressure exerted by fellow parents—is so overwhelming that they can barely think for themselves.
The facts they do manage to glean have usually been varnished or exaggerated or otherwise taken out of context to serve an agenda that isn’t their own.
Consider the parents of an eight-year-old girl named, say, Molly.
Molly’s parents know that Amy’s parents keep a gun in their house, so they have forbidden Molly to play there.
Molly’s parents feel good about having made such a smart choice to protect their daughter.
In a given year, there is one drowning of a child for every 11,000 residential pools in the United States.
(In a country with 6 million pools, this means that roughly 550 children under the age of ten drown each year.)
But most of us are, like Molly’s parents, terrible risk assessors.
Peter Sandman, a self-described “risk communications consultant” in Princeton, New Jersey, made this point in early 2004 after a single case of mad-cow disease in the United States prompted an antibeef frenzy.
“The basic reality,” Sandman told the New York Times, “is that the risks that scare people and the risks that kill people are very different.” Sandman offered a comparison between mad-cow disease (a superscary but exceedingly rare threat) and the spread of food-borne pathogens in the average home kitchen (exceedingly common but somehow not very scary).
“Risks that you control are much less a source of outrage than risks that are out of your control,” Sandman said.
I can clean the floor.” Sandman’s “control” principle might also explain why most people are more scared of flying in an airplane than driving a car.
So which should we actually fear more, flying or driving?
But the fear of death needs to be narrowed down.
So it’s the imminent possibility of death that drives the fear—which means that the most sensible way to calculate fear of death would be to think about it on a per-hour basis.
If you are taking a trip and have the choice of driving or flying, you might wish to consider the per-hour death rate of driving versus flying.
The per-hour death rate of driving versus flying, however, is about equal.
That is why experts rely on it; in a world that is increasingly impatient with long-term processes, fear is a potent short-term play.
Imagine that you are a government official charged with procuring the funds to fight one of two proven killers: terrorist attacks and heart disease.
The likelihood of any given person being killed in a terrorist attack is far smaller than the likelihood that the same person will clog up his arteries with fatty food and die of heart disease.
But a terrorist attack happens now; death by heart disease is some distant, quiet catastrophe.
Death by terrorist attack (or mad-cow disease) is considered wholly dreadful; death by heart disease is, for some reason, not.
Sandman is an expert who works both sides of the aisle.
Sandman has reduced his expertise to a tidy equation: Risk = hazard + outrage.
For the CEO with the bad hamburger meat, Sandman engages in “outrage reduction” for the environmentalists, it’s “outrage increase.” Note that Sandman addresses the outrage but not the hazard itself.
He concedes that outrage and hazard do not carry equal weight in his risk equation.
“When hazard is high and outrage is low, people underreact,” he says.
“And when hazard is low and outrage is high, they overreact.” So why is a swimming pool less frightening than a gun?
The thought of a child being shot through the chest with a neighbor ’s gun is gruesome, dramatic, horrifying—in a word, outrageous.
Swimming pools do not inspire outrage.
The steps to prevent drowning, meanwhile, are pretty straightforward: a watchful adult, a fence around the pool, a locked back door so a toddler doesn’t slip outside unnoticed.
Nevertheless, many parents so magnify the benefit of a car seat that they trek to the local police station or firehouse to have it installed just right.
Theirs is a gesture of love, surely, but also a gesture of what might be called obsessive parenting.
(Obsessive parents know who they are and are generally proud of the fact; non-obsessive parents also know who the obsessives are and tend to snicker at them.)
These products are often a response to some growing scare in which, as Peter Sandman might put it, the outrage outweighs the hazard.
Compare the four hundred lives that a few swimming pool precautions might save to the number of lives saved by far noisier crusades: child-resistant packaging (an estimated fifty lives a year), flameretardant pajamas (ten lives), keeping children away from airbags in cars (fewer than five young children a year have been killed by airbags since their introduction), and safety drawstrings on children’s clothing (two lives).
What does it matter if parents are manipulated by experts and marketers?
Shouldn’t we applaud any effort, regardless of how minor or manipulative, that makes even one child safer?
Don’t parents already have enough to worry about?
After all, parents are responsible for one of the most awesomely important feats we know: the very shaping of a child’s character.
The most radical shift of late in the conventional wisdom on parenting has been provoked by one simple question: how much do parents really matter?
As the link between abortion and crime makes clear, unwanted children—who are disproportionately subject to neglect and abuse—have worse outcomes than children who were eagerly welcomed by their parents.
But how much can those eager parents actually accomplish for their children’s sake?
But how then to explain another famous study, the Colorado Adoption Project, which followed the lives of 245 babies put up for adoption and found virtually no correlation between the child’s personality traits and those of his adopted parents?
The Nurture Assumption was in effect an attack on obsessive parenting, a book so provocative that it required two subtitles: Why Children Turn Out the Way They Do and Parents Matter Less than You Think and Peers Matter More.
Harris argued, albeit gently, that parents are wrong to think they contribute so mightily to their child’s personality.
This belief, she wrote, was a “cultural myth.” Harris argued that the top-down influence of parents is overwhelmed by the grassroots effect of peer pressure, the blunt force applied each day by friends and schoolmates.
“Patients in traditional forms of psychotherapy while away their fifty minutes reliving childhood conflicts and learning to blame their unhappiness on how their parents treated them,” Pinker wrote.
“Many biographies scavenge through the subject’s childhood for the roots of the grown-up’s tragedies and triumphs.
‘Parenting experts’ make women feel like ogres if they slip out of the house to work or skip a reading of Goodnight Moon.
Besides, even if peers exert so much influence on a child, isn’t it the parents who essentially choose a child’s peers?
Isn’t that why parents agonize over the right neighborhood, the right school, the right circle of friends?
Still, the question of how much parents matter is a good one.
his moral behavior?
And what weight should we assign each of the many inputs that affect a child’s outcome: genes, family environment, socioeconomic level, schooling, discrimination, luck, illness, and so on?
The white boy is raised in a Chicago suburb by parents who read widely and involve themselves in school reform.
His mother is a housewife who will eventually go back to college and earn a bachelor ’s degree in education.
His teachers think he may be a bona fide math genius.
His parents encourage him and are terribly proud when he skips a grade.
The black boy is born in Daytona Beach, Florida, and his mother abandons him at the age of two.
He makes sure to be asleep by the time his father comes home from drinking, and to be out of the house before his father awakes.
The father eventually goes to jail for sexual assault.
By the age of twelve, the boy is essentially fending for himself.
You don’t have to believe in obsessive parenting to think that the second boy doesn’t stand a chance and that the first boy has it made.
What are the odds that the second boy, with the added handicap of racial discrimination, will turn out to lead a productive life?
And how much of his fate should each boy attribute to his parents?
The first is that neither of us professes to be a parenting expert (although between us we do have six children under the age of five).
And since most parents would agree that education lies at the core of a child’s formation, it would make sense to begin by examining a telling set of school data.
Critics worry that school choice will leave behind the worst students in the worst schools.
That’s because the CPS, like most urban school districts, had a disproportionate number of minority students.
Despite the U.S. Supreme Court’s 1954 ruling in Brown v. Board of Education of Topeka, which dictated that schools be desegregated, many black CPS students continued to attend schools that were nearly all-black.
So in 1980 the U.S. Department of Justice and the Chicago Board of Education teamed up to try to better integrate the city’s schools.
Aside from its longevity, there are several reasons the CPS school-choice program is a good one to study.
It offers a huge data set—Chicago has the third-largest school system in the country, after New York and Los Angeles—as well as an enormous amount of choice (more than sixty high schools) and flexibility.
But the most serendipitous aspect of the CPS program—for the sake of a study, at least—is how the school-choice game was played.
The schools with good test scores and high graduation rates would be rabidly oversubscribed, making it impossible to satisfy every student’s request.
A behavioral scientist could hardly design a better experiment in his laboratory.
Now imagine multiplying those students by the thousands.
The answer will not be heartening to obsessive parents: in this case, school choice barely mattered at all.
What appears to be an advantage gained by going to a new school isn’t connected to the new school at all.
What this means is that the students—and parents—who choose to opt out tend to be smarter and more academically motivated to begin with.
But statistically, they gained no academic benefit by changing schools.
And is it true that the students left behind in neighborhood schools suffered?
No self-respecting parent, obsessive or otherwise, is ready to believe that.
But wait: maybe it’s because the CPS study measures high- school students; maybe by then the die has already been cast.
“There are too many students who arrive at high school not prepared to do high school work,” Richard P. Mills, the education commissioner of New York State, noted recently, “too many students who arrive at high school reading, writing, and doing math at the elementary level.
In other words, the black-white income gap is largely a product of a black-white education gap that could have been observed many years earlier.
“Reducing the black-white test score gap,” wrote the authors of one study, “would do more to promote racial equality than any other strategy that commands broad political support.” So where does that black-white test gap come from?
Many theories have been put forth over the years: poverty, genetic makeup, the “summer setback” phenomenon (blacks are thought to lose more ground than whites when school is out of session), racial bias in testing or in teachers’ perceptions, and a black backlash against “acting white.” In a paper called “The Economics of ‘Acting White,’” the young black Harvard economist Roland G. Fryer Jr. argues that some black students “have tremendous disincentives to invest in particular behaviors (i.e., education, ballet, etc.)
Such a label, in some neighborhoods, can carry penalties that range from being deemed a social outcast, to being beaten or killed.” Fryer cites the recollections of a young Kareem Abdul-Jabbar, known then as Lew Alcindor, who had just entered the fourth grade in a new school and discovered that he was a better reader than even the seventh graders: “When the kids found this out, I became a target….It was my first time away from home, my first experiencein an all-black situation, and I found myself being punished for everything I’d ever been taught was right.
I had to learn a new language simply to be able to deal with the threats.
I had good manners and was a good little boy and paid for it with my hide.” Fryer is also one of the authors of “Understanding the Black-White Test Score Gap in the First Two Years of School.” This paper takes advantage of a new trove of government data that helps reliably address the black-white gap.
The ECLS measured the students’ academic performance and gathered typical survey information about each child: his or her race, gender, family structure, socioeconomic status, the level of his or her parents’ education, and so on.
But the study went well beyond these basics.
It also included interviews with the students’ parents (and teachers and school administrators), posing a long list of questions more intimate than those in the typical government interview: whether the parents spanked their children, and how often; whether they took them to libraries or museums; how much television the children watched.
By subjecting it to the economist’s favorite trick: regression analysis.
No, regression analysis is not some forgotten form of psychiatric treatment.
In the case of the ECLS data, it might help to think of regression analysis as performing the following task: converting each of those twenty thousand schoolchildren into a sort of circuit board with an identical number of switches.
Each switch represents a single category of the child’s data: his first-grade math score, his third-grade math score, his first-grade reading score, his third-grade reading score, his mother ’s education level, his father ’s income, the number of books in his home, the relative affluence of his neighborhood, and so on.
He can line up all the children who share many characteristics—all the circuit boards that have their switches flipped the same direction—and then pinpoint the single characteristic they don’t share.
This is how he isolates the true impact of that single switch on the sprawling circuit board.
Let’s say that we want to ask the ECLS data a fundamental question about parenting and education: does having a lot of books in your home lead your child to do well in school?
A regression analysis can demonstrate correlation, but it doesn’t prove cause.
Perhaps the number of books in a child’s home merely indicates how much money his parents make.
It should be said that regression analysis is more art than science.
In the case of an academic study such as the ECLS, a researcher might control for any number of disadvantages that one student might carry when measured against the average student.)
After controlling for just a few variables—including the income and education level of the child’s parents and the mother ’s age at the birth of her first child—the gap between black and white children is virtually eliminated at the time the children enter school.
The data reveal that black children who perform poorly in school do so not because they are black but because a black child is more likely to come from a low-income, low-education household.
A typical black child and white child from the same socioeconomic background, however, have the same abilities in math and reading upon entering kindergarten.
First of all, because the average black child is more likely to come from a low-income, low-education household, the gap is very real: on average, black children still are scoring worse.
Worse yet, even when the parents’ income and education are controlled for, the black-white gap reappears within just two years of a child’s entering school.
Even fifty years after Brown v. Board, many American schools are virtually segregated.
The ECLS project surveyed roughly one thousand schools, taking samples of twenty children from each.
In 35 percent of those schools, not a single black child was included in the sample.
The typical white child in the ECLS study attends a school that is only 6 percent black; the typical black child, meanwhile, attends a school that is about 60 percent black.
Just how are the black schools bad?
Not, interestingly, in the ways that schools are traditionally measured.
In terms of class size, teachers’ education, and computer-to-student ratio, the schools attended by blacks and whites are similar.
These schools offer an environment that is simply not conducive to learning.
Black students are hardly the only ones who suffer in bad schools.
White children in these schools also perform poorly.
But all students in a bad school, black and white, do lose ground to students in good schools.
Consider this fact: the ECLS data reveal that black students in good schools don’t lose ground to their white counterparts, and black students in good schools outperform whites in poor schools.
Do the kids with PTA parents do better than the kids whose parents have never heard of the PTA?
The wide-ranging ECLS data offer a number of compelling correlations between a child’s personal circumstances and his school performance.
For instance, once all other factors are controlled for, it is clear that students from rural areas tend to do worse than average.
Suburban children, meanwhile, are in the middle of the curve, while urban children tend to score higher than average.
(It may be that cities attract a more educated workforce and, therefore, parents with smarter children.)
On average, girls test higher than boys, and Asians test higher than whites—although blacks, as we have already established, test similarly to whites from comparable backgrounds and in comparable schools.
Knowing what you now know about regression analysis, conventional wisdom, and the art of parenting, consider the following list of sixteen factors.
Keep in mind that these results reflect only a child’s early test scores, a useful but fairly narrow measurement; poor testing in early childhood isn’t necessarily a great harbinger of future earnings, creativity, or happiness.
The child has highly educated parents.
The child’s parents have high socioeconomic status.
The child’s parents recently moved into a better neighborhood.
The child attended Head Start.
The child’s parents speak English in the home.
The child’s parents regularly take him to museums.
The child’s parents are involved in the PTA.
The child frequently watches television.
The child’s parents read to him nearly every day.
Here now are the eight factors that are strongly correlated with test scores: The child has highly educated parents.
The child’s parents have high socioeconomic status.
The child’s parents speak English in the home.
The child’s parents are involved in the PTA.
The child’s parents recently moved into a better neighborhood.
The child attended Head Start.
The child’s parents regularly take him to museums.
The child frequently watches television.
The child’s parents read to him nearly every day.
Now, two by two: Matters: The child has highly educated parents.
A child whose parents are highly educated typically does well in school; not much surprise there.
Perhaps more important, parents with higher IQs tend to get more education, and IQ is strongly hereditary.
It should, however, offer encouragement to the roughly twenty million American schoolchildren being raised by a single parent.
Matters: The child’s parents have high socioeconomic status.
Doesn’t: The child’s parents recently moved into a better neighborhood.
A high socioeconomic status is strongly correlated to higher test scores, which seems sensible.
Socioeconomic status is a strong indicator of success in general—it suggests a higher IQ and more education—and successful parents are more likely to have successful children.
This mother tends to be a woman who wanted to get some advanced education or develop traction in her career.
She is also likely to want a child more than a teenage mother wants a child.
This doesn’t mean that an older first-time mother is necessarily a better mother, but she has put herself—and her children—in a more advantageous position.
(It is worth noting that this advantage is nonexistent for a teenage mother who waits until she is thirty to have her second child.
At the same time, a mother who stays home from work until her child goes to kindergarten does not seem to provide any advantage.
Obsessive parents might find this lack of correlation bothersome—what was the point of all those Mommy and Me classes?—but that is what the data tell us.
Doesn’t: The child attended Head Start.
A low-birthweight child, in turn, is more likely to be a poor child—and, therefore, more likely to attend Head Start, the federal preschool program.
But according to the ECLS data, Head Start does nothing for a child’s future test scores.
Despite a deep reservoir of appreciation for Head Start (one of this book’s authors was a charter student), we must acknowledge that it has repeatedly been proven ineffectual in the long term.
Here’s a likely reason: instead of spending the day with his own undereducated, overworked mother, the typical Head Start child spends the day with someone else’s undereducated, overworked mother.
As it happens, fewer than 30 percent of Head Start teachers have even a bachelor ’s degree.
And the job pays so poorly—about $21,000 for a Head Start teacher versus $40,000 for the average public-school kindergarten teacher—that it is unlikely to attract better teachers any time soon.
Matters: The child’s parents speak English in the home.
Doesn’t: The child’s parents regularly take him to museums.
A child with English-speaking parents does better in school than one whose parents don’t speak English.
This correlation is further supported by the performance of Hispanic students in the ECLS study.
As a group, Hispanic students test poorly; they are also disproportionately likely to have non-English-speaking parents.
Culture cramming may be a foundational belief of obsessive parenting, but the ECLS data show no correlation between museum visits and test scores.
There is a strong correlation—a negative one—between adoption and school test scores.
Studies have shown that a child’s academic abilities are far more influenced by the IQs of his biological parents than the IQs of his adoptive parents, and mothers who offer up their children for adoption tend to have significantly lower IQs than the people who are doing the adopting.
There is another explanation for low-achieving adoptees which, though it may seem distasteful, jibes with the basic economic theory of self-interest: a woman who knows she will offer her baby for adoption may not take the same prenatal care as a woman who is keeping her baby.
We might therefore assume that parents who spank are unenlightened in other ways.
Remember, the ECLS survey included direct interviews with the children’s parents.
It may be that honesty is more important to good parenting than spanking is to bad parenting.
Matters: The child’s parents are involved in the PTA.
Doesn’t: The child frequently watches television.
A child whose parents are involved in the PTA tends to do well in school—which probably indicates that parents with a strong relationship to education get involved in the PTA, not that their PTA involvement somehow makes their children smarter.
The ECLS data show no correlation, meanwhile, between a child’s test scores and the amount of television he watches.
Despite the conventional wisdom, watching television apparently does not turn a child’s brain to mush.
(In Finland, whose education system has been ranked the world’s best, most children do not begin school until age seven but have often learned to read on their own by watching American television with Finnish subtitles.)
Doesn’t: The child’s parents read to him nearly every day.
But regularly reading to a child doesn’t affect early childhood test scores.
It bounces us back to our original question: just how much, and in what ways, do parents really matter?
If reading books doesn’t have an impact on early childhood test scores, could it be that the books’ mere physical presence in the house makes the children smarter?
Here’s a likely theory: most parents who buy a lot of children’s books tend to be smart and well educated to begin with.
Or perhaps they care a great deal about education, and about their children in general.
(Which means they create an environment that encourages and rewards learning.)
Such parents may believe—as fervently as the governor of Illinois believed—that every children’s book is a talisman that leads to unfettered intelligence.
So what does all this have to say about the importance of parents in general?
Consider again the eight ECLS factors that are correlated with school test scores: The child has highly educated parents.
The child’s parents have high socioeconomic status.
The child’s parents speak English in the home.
The child’s parents are involved in the PTA.
The child’s parents recently moved into a better neighborhood.
The child attended Head Start.
The child’s parents regularly take him to museums.
The child frequently watches television.
The child’s parents read to him nearly every day.
To overgeneralize a bit, the first list describes things that parents are; the second list describes things that parents do.
Parents who are well educated, successful, and healthy tend to have children who test well in school; but it doesn’t seem to much matter whether a child is trotted off to museums or spanked or sent to Head Start or frequently read to or plopped in front of the television.
For parents—and parenting experts—who are obsessed with child-rearing technique, this may be sobering news.
But this is not to say that parents don’t matter.
In this regard, an overbearing parent is a lot like a political candidate who believes that money wins elections—whereas in truth, all the money in the world can’t get a candidate elected if the voters don’t like him to start with.
He used three adoption studies, two American and one British, each of them containing indepth data about the adopted children, their adoptive parents, and their biological parents.
Sacerdote found that parents who adopt children are typically smarter, better educated, and more highly paid than the baby’s biological parents.
But the adoptive parents’ advantages had little bearing on the child’s school performance.
As also seen in the ECLS data, adopted children test relatively poorly in school; any influence the adoptive parents might exert is seemingly outweighed by the force of genetics.
But, Sacerdote found, the parents were not powerless forever.
Compared to similar children who were not put up for adoption, the adoptees were far more likely to attend college, to have a well-paid job, and to wait until they were out of their teens before getting married.
It was the influence of the adoptive parents, Sacerdote concluded, that made the difference.
The belief in parental power is manifest in the first official act a parent commits: giving the baby a name.
Many parents seem to believe that a child cannot prosper unless it is hitched to the right name; names are seen to carry great aesthetic or even predictive powers.
He went to prep school on a scholarship, graduated from Lafayette College in Pennsylvania, and joined the New York Police Department (this was his mother ’s longtime wish), where he made detective and, eventually, sergeant.
“So I have a bunch of names,” he says today, “from Jimmy to James to whatever they want to call you.
But they rarely call you Loser.” Once in a while, he said, “they throw a French twist on it: ‘Losier.’” To his police colleagues, he is known as Lou.
The most noteworthy achievement of Winner Lane, now in his midforties, is the sheer length of his criminal record: nearly three dozen arrests for burglary, domestic violence, trespassing, resisting arrest, and other mayhem.
Then there is the recent case of Temptress, a fifteen-year-old girl whose misdeeds landed her in Albany County Family Court in New York.
The judge, W. Dennis Duggan, had long taken note of the strange names borne by some offenders.
One teenage boy, Amcher, had been named for the first thing his parents saw upon reaching the hospital: the sign for Albany Medical Center Hospital Emergency Room.
But Duggan considered Temptress the most outrageous name he had come across.
* It isn’t much of a stretch to assume that Temptress didn’t have ideal parents.
People who can’t be bothered to come up with a name for their child aren’t likely to be the best parents either.
Is this fact merely a curiosity or does it have something larger to say about names and culture?
After graduate work at Penn State and the University of Chicago, he was hired as a Harvard professor at age twenty-five.
Fryer ’s mission is the study of black underachievement.
I basically want to figure out where blacks went wrong, and I want to devote my life to this.” In addition to economic and social disparity between blacks and whites, Fryer had become intrigued by the virtual segregation of culture.
Blacks and whites watch different television shows.
(Monday Night Football is the only show that typically appears on each group’s top ten list; Seinfeld, one of the most popular sitcoms in history, never ranked in the top fifty among blacks.)
(Newports enjoy a 75 percent market share among black teenagers versus 12 percent among whites; the white teenagers are mainly smoking Marlboros.)
And black parents give their children names that are starkly different from white children’s.
Fryer came to wonder: is distinctive black culture a cause of the economic disparity between blacks and whites or merely a reflection of it?
As with the ECLS study, Fryer went looking for the answer in a mountain of data: birthcertificate information for every child born in California since 1961.
The data, covering more than sixteen million births, included standard items such as name, gender, race, birth-weight, and the parents’ marital status, as well as more telling factors about the parents: their zip code (which indicates socioeconomic status and a neighborhood’s racial composition), their means of paying the hospital bill (again, an economic indicator), and their level of education.
The California data prove just how dissimilarly black and white parents name their children.
White and Asian-American parents, meanwhile, give their children remarkably similar names; there is some disparity between white and Hispanic-American parents, but it is slim compared to the black- white naming gap.
Until the early 1970s, there was a great overlap between black and white names.
(Boys’ names moved in the same direction but less aggressively—probably because parents of all races are less adventurous with boys’ names than with girls’.)
Given the location and timing of this change—dense urban areas where Afro-American activism was gathering strength—the most likely cause of the explosion in distinctively black names was the Black Power movement, which sought to accentuate African culture and fight claims of black inferiority.
A great many black names today are unique to blacks.
Even more remarkably, nearly 30 percent of the black girls are given a name that is unique among the names of every baby, white and black, born that year in California.
Even among very popular black names, there is little overlap with whites.
The data offer a clear answer: an unmarried, low-income, undereducated teenage mother from a black neighborhood who has a distinctively black name herself.
“If I start naming my kid Madison,” he says, “you might think, ‘Oh, you want to go live across the railroad tracks, don’t you?’” If black kids who study calculus and ballet are thought to be “acting white,” Fryer says, then mothers who call their babies Shanice are simply “acting black.” The California study shows that many white parents send as strong a signal in the opposite direction.
More than 40 percent of the white babies are given names that are at least four times more common among whites.
In one recent ten-year stretch, each of these names was given to at least two thousand babies in California—fewer than 2 percent of them black.
So what are the “whitest” names and the “blackest” names?
Over the years, a series of “audit studies” have tried to measure how people perceive different names.
In a typical audit study, a researcher would send two identical (and fake) résumés, one with a traditionally white name and the other with an immigrant or minority-sounding name, to potential employers.
The “white” résumés have always gleaned more job interviews.
According to such a study, if DeShawn Williams and Jake Williams sent identical résumés to the same employer, Jake Williams would be more likely to get a callback.
The implication is that blacksounding names carry an economic penalty.
Or did he reject him because “DeShawn” sounds like someone from a low-income, low-education family?
A résumé is a fairly undependable set of clues— a recent study showed that more than 50 percent of them contain lies—so “DeShawn” may simply signal a disadvantaged background to an employer who believes that workers from such backgrounds are undependable.
Or is the interview a painful and discouraging waste of time for the black applicant—that is, an economic penalty for having a white-sounding name?
Along those same lines, perhaps a black person with a white name pays an economic penalty in the black community; and what of the potential advantage to be gained in the black community by having a distinctively black name?
Some people change names for economic purposes: after a New York livery-cab driver named Michael Goldberg was shot in early 2004, it was reported that Mr. Goldberg was in fact an Indian-born Sikh who thought it advantageous to take a Jewish name upon immigrating to New York.
Goldberg’s decision might have puzzled some people in show business circles, where it is a time-honored tradition to change Jewish names.
Thus did Issur Danielovitch become Kirk Douglas; thus did the William Morris Agency rise to prominence under its namesake, the former Zelman Moses.
It is tempting to think so—just as it is tempting to think that a truckload of children’s books will make a child smarter.
Though the audit studies can’t be used to truly measure how much a name matters, the California names data can.
The California data included not only each baby’s vital statistics but information about the mother ’s level of education, income, and, most significantly, her own date of birth.
Now a new and extremely potent story emerged from the data: it was possible to track the life outcome of any individual woman.
This is the sort of data chain that researchers dream about, making it possible to identify a set of children who were born under similar circumstances, then locate them again twenty or thirty years later to see how they turned out.
Among the hundreds of thousands of such women in the California data, many bore distinctively black names and many others did not.
Using regression analysis to control for other factors that might influence life trajectories, it was then possible to measure the impact of a single factor—in this case, a woman’s first name—on her educational, income, and health outcomes.
The data show that, on average, a person with a distinctively black name—whether it is a woman named Imani or a man named DeShawn—does have a worse life outcome than a woman named Molly or a man named Jake.
But it isn’t the fault of their names.
If two black boys, Jake Williams and DeShawn Williams, are born in the same neighborhood and into the same familial and economic circumstances, they would likely have similar life outcomes.
But the kind of parents who name their son Jake don’t tend to live in the same neighborhoods or share economic circumstances with the kind of parents who name their son DeShawn.
And that’s why, on average, a boy named Jake will tend to earn more money and get more education than a boy named DeShawn.
A DeShawn is more likely to have been handicapped by a low-income, low-education, single-parent background.
Here’s a guess: anybody who bothers to change his name in the name of economic success is—like the high-school freshmen in Chicago who entered the school-choice lottery—at least highly motivated, and motivation is probably a stronger indicator of success than, well, a name.
Just as the ECLS data answered questions about parenting that went well beyond the black-white test gap, the California names data tell a lot of stories in addition to the one about distinctively black names.
Broadly speaking, the data tell us how parents see themselves—and, more significantly, what kind of expectations they have for their children.
Not, that is, the actual source of the name—that much is usually obvious: there’s the Bible, there’s the huge cluster of traditional English and Germanic and Italian and French names, there are princess names and hippie names, nostalgic names and place names.
Increasingly, there are brand names (Lexus, Armani, Bacardi, Timberland) and what might be called aspirational names.
Then there are the invented names.
Roland G. Fryer Jr., while discussing his names research on a radio show, took a call from a black woman who was upset with the name just given to her baby niece.
It was pronounced shuh-TEED but was in fact spelled “Shithead.”* Shithead has yet to catch on among the masses, but other names do.
We all know that names rise and fall and rise—witness the return of Sophie and Max from near extinction—but is there a discernible pattern to these movements?
Among the most interesting revelations in the data is the correlation between a baby’s name and the parents’ socioeconomic status.
Consider the most common female names found in middle-income white households versus low-income white households.
But keep in mind that these are the most common names of all, and consider the size of the data set.
Five names in each category don’t appear at all in the other category’s top twenty.
Here are the top five names among high-end and low-end families, in order of their relative disparity with the other category: Most Common High-End White Girl Names 1.
Robert Considering the relationship between income and names, and given the fact that income and education are strongly correlated, it is not surprising to find a similarly strong link between the parents’ level of education and the name they give their baby.
Once again drawing from the pool of most common names among white children, here are the top picks of highly educated parents versus those with the least education: Most Common White Girl Names Among High-Education Parents 1.
Tyler The effect is even more pronounced when the sample is widened beyond the most common names.
Drawing from the entire California database, here are the names that signify the most poorly educated white parents.
The Twenty White Girl Names That Best Signify Low-Education Parents* (Average number of years of mother ’s education in parentheses) 1.
Brandi (12.17) If you or someone you love is named Cindy or Brenda and is over, say, forty, and feels that those names did not formerly connote a low-education family, you are right.
These names, like many others, have shifted hard and fast of late.
Some of the other low-education names are obviously misspellings, whether intentional or not, of more standard names.
In most cases the standard spellings of the names—Tabitha, Cheyenne, Tiffany, Brittany, and Jasmine—also signify low education.
But the various spellings of even one name can reveal a strong disparity: Ten “Jasmines” in Ascending Order of Maternal Education (Years of mother ’s education in parentheses) 1.
Jasmyn (13.23) Here is the list of low-education white boy names.
The Twenty White Boy Names That Best Signify Low-Education Parents* (Years of mother ’s education in parentheses) 1.
Harley (12.22) Now for the names that signify the highest level of parental education.
These names don’t have much in common, phonetically or aesthetically, with the low-education names.
The girls’ names are in most regards diverse, though with a fair share of literary and otherwise artful touches.
A caution to prospective parents who are shopping for a “smart” name: remember that such a name won’t make your child smart; it will, however, give her the same name as other smart kids—at least for a while.
(For a much longer and more varied list of girls’ and boys’ names) The Twenty White Girl Names That Best Signify High-Education Parents* (Years of mother ’s education in parentheses) 1.
Neeka (15.77) Now for the boys’ names that are turning up these days in high-education households.
This list is particularly heavy on the Hebrew, with a noticeable trend toward Irish traditionalism.
The Twenty White Boy Names That Best Signify High-Education Parents* (Years of mother ’s education in parentheses) 1.
Calder (15.75) If many names on the above lists were unfamiliar to you, don’t feel bad.
Even boys’ names— which have always been scarcer than girls’—have been proliferating wildly.
This means that even the most popular names today are less popular than they used to be.
Consider the ten most popular names given to black baby boys in California in 1990 and then in 2000.
Justin (141) In the space of ten years, even the most popular name among black baby boys (532 occurrences for Michael) became far less popular (308 occurrences for Isaiah).
So parents are plainly getting more diverse with names.
Note that four of the 1990 names (James, Robert, David, and Kevin) fell out of the top ten by 2000.
But the names that replaced them in 2000 weren’t bottom dwellers.
Three of the new names—Isaiah, Jordan, and Elijah—were in fact numbers one, two, and three in 2000.
For an even more drastic example of how quickly and thoroughly a name can cycle in and out of use, consider the ten most popular names given to white girls in California in 1960 and then in 2000.
Megan Not a single name from 1960 remains in the top ten.
So how about comparing today’s most popular names with the top ten from only twenty years earlier?
Megan A single holdover: Sarah.
* It’s easy enough to see that new names become very popular very fast—but why?
Here are the most popular names given to baby girls in the 1990s among low-income families and among families of middle income or higher.
You might want to compare these names with the “Most Popular White Girl Names” list, which includes the top ten overall names from 1980 and 2000.
Lauren and Madison, two of the most popular “high-end” names from the 1990s, made the 2000 top ten list.
Amber and Heather, meanwhile, two of the overall most popular names from 1980, are now among the “low-end” names.
There is a clear pattern at play: once a name catches on among high-income, highly educated parents, it starts working its way down the socioeconomic ladder.
Amber and Heather started out as high-end names, as did Stephanie and Brittany.
For every high-end baby named Stephanie or Brittany, another five lower-income girls received those names within ten years.
But celebrities actually have a weak effect on baby names.
As of 2000, the pop star Madonna had sold 130 million records worldwide but hadn’t generated even the ten copycat namings—in California, no less—required to make the master index of four thousand names from which the sprawling list of girls’ names was drawn.
(It should also be noted that many girls’ names, including Shirley, Carol, Leslie, Hilary, Renee, Stacy, and Tracy began life as boys’ names, but girls’ names almost never cross over to boys.)
Parents are reluctant to poach a name from someone too near—family members or close friends—but many parents, whether they realize it or not, like the sound of names that sound “successful.” But as a high-end name is adopted en masse, high-end parents begin to abandon it.
Eventually, it is considered so common that even lower-end parents may not want it, whereby it falls out of the rotation entirely.
The lower-end parents, meanwhile, go looking for the next name that the upper-end parents have broken in.
So the implication is clear: the parents of all those Alexandras, Laurens, Katherines, Madisons, and Rachels should not expect the cachet to last much longer.
Those names are already on their way to overexposure.
Where, then, will the new high-end names come from?
It wouldn’t be surprising to find them among the “smartest” girls’ and boys’ names in California, listed on pages 181–82, that are still fairly obscure.
The same could be surmised of most of the Hebrew names (Rotem and Zofia, Akiva and Zev), even though many of today’s most mainstream names (David, Jonathan, Samuel, Benjamin, Rachel, Hannah, Sarah, Rebecca) are of course Hebrew biblical names.
Aviva may be the one modern Hebrew name that is ready to break out: it’s easy to pronounce, pretty, peppy, and suitably flexible.
Drawn from a pair of “smart” databases, here is a sampling of today’s high-end names.
Some of them, as unlikely as it seems, are bound to become tomorrow’s mainstream names.
Aidan Aldo Anderson Ansel Asher Beckett Bennett Carter Cooper Finnegan Harper Jackson Johan Keyon Liam Maximilian McGregor Oliver Reagan Sander Sumner Will Obviously, a variety of motives are at work when parents consider a name for their child.
It would be an overstatement to suggest that all parents are looking—whether consciously or not—for a “smart” name or a “high-end” name.
What the California names data suggest is that an overwhelming number of parents use a name to signal their own expectations of how successful their children will be.
But the parents can at least feel better knowing that, from the very outset, they tried their best.
EPILOGUE: Two Paths to Harvard And now, with all these pages behind us, an early promise has been confirmed: this book indeed has no “unifying theme.” But if there is no unifying theme to Freakonomics, there is at least a common thread running through the everyday application of Freakonomics.
Perhaps you’ll put up a sturdy gate around your swimming pool or push your real-estate agent to work a little harder.
You might become more skeptical of the conventional wisdom; you may begin looking for hints as to how things aren’t quite what they seem; perhaps you will seek out some trove of data and sift through it, balancing your intelligence and your intuition to arrive at a glimmering new idea.
To claim that legalized abortion resulted in a massive drop in crime will inevitably lead to explosive moral reactions.
But the fact of the matter is that Freakonomics-style thinking simply doesn’t traffic in morality.
As we suggested near the beginning of this book, if morality represents an ideal world, then economics represents the actual world.
Consider the question posed at the beginning of this book’s penultimate chapter: how much do parents really matter?
The data have by now made it clear that parents matter a great deal in some regards (most of which have been long determined by the time a child is born) and not at all in others (the ones we obsess about).
You can’t blame parents for trying to do something—anything—to help their child succeed, even if it’s something as irrelevant as giving him a high-end first name.
If you are in any way typical, you have known some intelligent and devoted parents whose child went badly off the rails.
You may have also known of the opposite instance, where a child succeeds despite his parents’ worst intentions and habits.
The white boy who grew up outside Chicago had smart, solid, encouraging, loving parents who stressed education and family.
The second child, now twenty-eight years old, is Roland G. Fryer Jr., the Harvard economist studying black underachievement.
